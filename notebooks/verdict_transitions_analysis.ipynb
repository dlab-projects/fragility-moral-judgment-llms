{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Verdict Transition Analysis\n\nThis notebook analyzes verdict transitions (flips) across different datasets:\n1. **Content Perturbations** - psychological, presentation, robustness perturbations\n2. **Entropy Data** - sampling variance (m=15 samples)\n3. **Protocol Tests** - explanation_first, system_prompt, unstructured\n4. **Reasoning Models** - o3-mini, claude-thinking, deepseek-r1, qwq-32b\n\n## Key Concepts\n\n**Verdict Categories:**\n- `Self_At_Fault` (YTA) - OP is at fault\n- `Other_At_Fault` (NTA) - Other party is at fault\n- `All_At_Fault` (ESH) - Everyone shares blame\n- `No_One_At_Fault` (NAH) - No one is at fault\n\n**OP Blame Status Framework:**\n\nWe classify verdicts into two blame-status groups:\n- **Narrator-implicated**: Self_At_Fault (YTA), All_At_Fault (ESH)\n- **Narrator-exonerated**: Other_At_Fault (NTA), No_One_At_Fault (NAH)\n\n**Transition Classification:**\n- **OP-status preserved**: Transition stays within the same blame-status group\n  - YTA↔ESH (both implicate OP)\n  - NTA↔NAH (both exonerate OP)\n- **OP-status reversed**: Transition crosses between groups (complete culpability reversal)\n  - YTA↔NTA, YTA↔NAH, ESH↔NTA, ESH↔NAH\n\n**Directional Decomposition:**\n- **OP newly blamed**: NTA→{YTA,ESH}, NAH→{YTA,ESH}\n- **OP exonerated**: YTA→{NTA,NAH}, ESH→{NTA,NAH}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path('../../')\n",
    "DATA_DIR = Path('../data')\n",
    "RESULTS_DIR = Path('../data')\n",
    "FIGURES_DIR = PROJECT_ROOT / 'figures'\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core utility functions\n\nVALID_VERDICTS = ['Self_At_Fault', 'Other_At_Fault', 'All_At_Fault', 'No_One_At_Fault']\n\nSHORT_NAMES = {\n    'Self_At_Fault': 'YTA',\n    'Other_At_Fault': 'NTA',\n    'All_At_Fault': 'ESH',\n    'No_One_At_Fault': 'NAH'\n}\n\nAITA_TO_SEMANTIC = {\n    'YTA': 'Self_At_Fault',\n    'NTA': 'Other_At_Fault',\n    'ESH': 'All_At_Fault',\n    'NAH': 'No_One_At_Fault',\n    'INFO': 'Unclear'\n}\n\n# OP Blame Status Framework\nOP_BLAMED = {'Self_At_Fault', 'All_At_Fault'}  # Narrator-implicated\nOP_EXONERATED = {'Other_At_Fault', 'No_One_At_Fault'}  # Narrator-exonerated\n\n\ndef get_op_status(verdict):\n    \"\"\"Return whether OP is blamed or exonerated.\"\"\"\n    if verdict in OP_BLAMED:\n        return 'blamed'\n    elif verdict in OP_EXONERATED:\n        return 'exonerated'\n    return None\n\n\ndef get_op_status_preserved(base_v, target_v):\n    \"\"\"Check if OP blame status is preserved across transition.\"\"\"\n    base_status = get_op_status(base_v)\n    target_status = get_op_status(target_v)\n    if base_status is None or target_status is None:\n        return None\n    return base_status == target_status\n\n\ndef get_op_direction(base_v, target_v):\n    \"\"\"Classify transition by directional OP blame status change.\"\"\"\n    base_blamed = base_v in OP_BLAMED\n    target_blamed = target_v in OP_BLAMED\n    \n    if base_blamed and target_blamed:\n        return 'OP_stays_blamed'\n    elif not base_blamed and not target_blamed:\n        return 'OP_stays_exonerated'\n    elif not base_blamed and target_blamed:\n        return 'OP_newly_blamed'\n    else:  # base_blamed and not target_blamed\n        return 'OP_exonerated'\n\n\ndef analyze_transitions_op_status(flips_df, base_col, target_col, label=\"\"):\n    \"\"\"Analyze transitions using OP blame status framework.\"\"\"\n    if len(flips_df) == 0:\n        print(f\"{label}: No flips found\")\n        return None\n    \n    df = flips_df.copy()\n    df['transition'] = df[base_col] + ' → ' + df[target_col]\n    df['op_status_preserved'] = df.apply(lambda r: get_op_status_preserved(r[base_col], r[target_col]), axis=1)\n    df['op_direction'] = df.apply(lambda r: get_op_direction(r[base_col], r[target_col]), axis=1)\n    \n    n = len(df)\n    preserved = df['op_status_preserved'].sum()\n    reversed_ct = n - preserved\n    \n    # Directional breakdown\n    direction_counts = df['op_direction'].value_counts()\n    \n    result = {\n        'label': label,\n        'n_flips': n,\n        'preserved': preserved,\n        'reversed': reversed_ct,\n        'preserved_pct': preserved / n * 100,\n        'reversed_pct': reversed_ct / n * 100,\n        'OP_newly_blamed': direction_counts.get('OP_newly_blamed', 0),\n        'OP_exonerated': direction_counts.get('OP_exonerated', 0),\n        'OP_stays_blamed': direction_counts.get('OP_stays_blamed', 0),\n        'OP_stays_exonerated': direction_counts.get('OP_stays_exonerated', 0),\n        'net_toward_blame': direction_counts.get('OP_newly_blamed', 0) - direction_counts.get('OP_exonerated', 0),\n        'transitions': df['transition'].value_counts().to_dict(),\n        'df': df\n    }\n    \n    return result\n\n\ndef print_op_status_summary(result):\n    \"\"\"Print formatted OP status transition summary.\"\"\"\n    if result is None:\n        return\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"{result['label']}\")\n    print(f\"{'='*80}\")\n    print(f\"Total flips: {result['n_flips']}\")\n    \n    print(f\"\\nOP Blame Status:\")\n    print(f\"  Status preserved: {result['preserved']:5} ({result['preserved_pct']:5.1f}%)  (within-group: YTA↔ESH or NTA↔NAH)\")\n    print(f\"  Status reversed:  {result['reversed']:5} ({result['reversed_pct']:5.1f}%)  (cross-group: complete reversal)\")\n    \n    print(f\"\\nDirectional Breakdown:\")\n    print(f\"  OP newly blamed:     {result['OP_newly_blamed']:5}  (NTA/NAH → YTA/ESH)\")\n    print(f\"  OP exonerated:       {result['OP_exonerated']:5}  (YTA/ESH → NTA/NAH)\")\n    print(f\"  OP stays blamed:     {result['OP_stays_blamed']:5}  (YTA ↔ ESH)\")\n    print(f\"  OP stays exonerated: {result['OP_stays_exonerated']:5}  (NTA ↔ NAH)\")\n    \n    net = result['net_toward_blame']\n    direction = \"toward blame\" if net > 0 else \"toward exoneration\" if net < 0 else \"balanced\"\n    print(f\"\\n  NET FLOW: {net:+5} ({direction})\")\n    \n    # Ratio\n    if result['OP_exonerated'] > 0:\n        ratio = result['OP_newly_blamed'] / result['OP_exonerated']\n        print(f\"  Blame/Exon Ratio: {ratio:.2f}x\")\n    \n    print(f\"\\nAll transitions:\")\n    for t, c in sorted(result['transitions'].items(), key=lambda x: -x[1]):\n        parts = t.split(' → ')\n        direction = get_op_direction(parts[0], parts[1])\n        pct = c / result['n_flips'] * 100\n        print(f\"  {t:45} {c:5} ({pct:5.1f}%) [{direction}]\")\n\n\ndef compute_op_direction_by_category(flips_df, base_col, target_col, category_col, categories=None):\n    \"\"\"Compute OP direction breakdown for each category.\"\"\"\n    if categories is None:\n        categories = flips_df[category_col].unique()\n    \n    results = []\n    for cat in categories:\n        cat_flips = flips_df[flips_df[category_col] == cat]\n        if len(cat_flips) == 0:\n            continue\n        \n        cat_flips = cat_flips.copy()\n        cat_flips['op_direction'] = cat_flips.apply(\n            lambda r: get_op_direction(r[base_col], r[target_col]), axis=1\n        )\n        \n        dir_counts = cat_flips['op_direction'].value_counts()\n        n = len(cat_flips)\n        \n        newly_blamed = dir_counts.get('OP_newly_blamed', 0)\n        exonerated = dir_counts.get('OP_exonerated', 0)\n        \n        results.append({\n            'category': cat,\n            'n_flips': n,\n            'newly_blamed': newly_blamed,\n            'exonerated': exonerated,\n            'stays_blamed': dir_counts.get('OP_stays_blamed', 0),\n            'stays_exonerated': dir_counts.get('OP_stays_exonerated', 0),\n            'net': newly_blamed - exonerated,\n            'direction': 'toward_blame' if newly_blamed > exonerated else 'toward_exoneration'\n        })\n    \n    return pd.DataFrame(results)\n\n\n# Keep legacy functions for backwards compatibility\ndef get_transition_distance(v1, v2):\n    \"\"\"Legacy: Categorize transition by semantic distance (Hamming).\"\"\"\n    # 1-step: differ on one dimension\n    one_step = {\n        ('Self_At_Fault', 'All_At_Fault'),\n        ('All_At_Fault', 'Self_At_Fault'),\n        ('Other_At_Fault', 'No_One_At_Fault'),\n        ('No_One_At_Fault', 'Other_At_Fault'),\n        ('All_At_Fault', 'Other_At_Fault'),\n        ('Other_At_Fault', 'All_At_Fault'),\n        ('All_At_Fault', 'No_One_At_Fault'),\n        ('No_One_At_Fault', 'All_At_Fault'),\n    }\n    # 2-step: differ on both dimensions (diagonal)\n    two_step = {\n        ('Self_At_Fault', 'Other_At_Fault'),\n        ('Other_At_Fault', 'Self_At_Fault'),\n        ('Self_At_Fault', 'No_One_At_Fault'),\n        ('No_One_At_Fault', 'Self_At_Fault'),\n    }\n    if (v1, v2) in one_step:\n        return '1-step'\n    elif (v1, v2) in two_step:\n        return '2-step'\n    else:\n        return 'unknown'\n\n\ndef analyze_transitions(flips_df, base_col, target_col, label=\"\"):\n    \"\"\"Analyze transitions from a dataframe of flips (OP status framework).\"\"\"\n    return analyze_transitions_op_status(flips_df, base_col, target_col, label)\n\n\ndef print_transition_summary(result):\n    \"\"\"Print formatted transition summary (OP status framework).\"\"\"\n    return print_op_status_summary(result)\n\n\ndef compute_asymmetry(flips_df, base_col, target_col):\n    \"\"\"Compute transition asymmetry between verdict pairs.\"\"\"\n    pairs = [\n        ('Self_At_Fault', 'Other_At_Fault'),\n        ('Self_At_Fault', 'All_At_Fault'),\n        ('Other_At_Fault', 'All_At_Fault'),\n        ('Other_At_Fault', 'No_One_At_Fault'),\n        ('All_At_Fault', 'No_One_At_Fault'),\n        ('Self_At_Fault', 'No_One_At_Fault'),\n    ]\n    \n    results = []\n    for v1, v2 in pairs:\n        a_to_b = len(flips_df[(flips_df[base_col] == v1) & (flips_df[target_col] == v2)])\n        b_to_a = len(flips_df[(flips_df[base_col] == v2) & (flips_df[target_col] == v1)])\n        net = a_to_b - b_to_a\n        ratio = a_to_b / b_to_a if b_to_a > 0 else float('inf')\n        \n        results.append({\n            'pair': f\"{SHORT_NAMES[v1]}↔{SHORT_NAMES[v2]}\",\n            'v1': v1,\n            'v2': v2,\n            'a_to_b': a_to_b,\n            'b_to_a': b_to_a,\n            'net': net,\n            'ratio': ratio\n        })\n    \n    return pd.DataFrame(results)\n\n\ndef print_asymmetry_table(asym_df):\n    \"\"\"Print formatted asymmetry table.\"\"\"\n    print(f\"\\n{'Pair':<12} | {'A→B':>5} | {'B→A':>5} | {'Net':>6} | {'Ratio':>6}\")\n    print(\"-\" * 50)\n    for _, row in asym_df.iterrows():\n        ratio_str = f\"{row['ratio']:.2f}x\" if row['ratio'] != float('inf') else \"inf\"\n        print(f\"{row['pair']:<12} | {row['a_to_b']:>5} | {row['b_to_a']:>5} | {row['net']:>+6} | {ratio_str:>6}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Content Perturbations Analysis\n",
    "\n",
    "Analysis of verdict transitions from baseline to perturbed scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load content perturbations data\n",
    "content_df = pd.read_parquet(DATA_DIR / 'content_eval.parquet')\n",
    "print(f\"Content perturbations data: {content_df.shape}\")\n",
    "print(f\"Perturbation types: {content_df['perturbation_type'].unique()}\")\n",
    "print(f\"Models: {content_df['model'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid flips\n",
    "content_flips = content_df[\n",
    "    (content_df['verdict_flipped'] == True) & \n",
    "    (content_df['base_verdict'].isin(VALID_VERDICTS)) &\n",
    "    (content_df['standardized_judgment'].isin(VALID_VERDICTS)) &\n",
    "    (content_df['perturbation_type'] != 'none')\n",
    "].copy()\n",
    "\n",
    "n_total = len(content_df[content_df['perturbation_type'] != 'none'])\n",
    "n_flips = len(content_flips)\n",
    "print(f\"Total perturbed rows: {n_total}\")\n",
    "print(f\"Total flips: {n_flips} ({n_flips/n_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall content perturbation transitions\n",
    "content_result = analyze_transitions(content_flips, 'base_verdict', 'standardized_judgment', \n",
    "                                     \"CONTENT PERTURBATIONS - ALL\")\n",
    "print_transition_summary(content_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By perturbation category\n",
    "categories = {\n",
    "    'psychological': ['push_nta_self_justifying', 'push_nta_social_proof', 'push_nta_victim_pattern',\n",
    "                      'push_yta_pattern_admission', 'push_yta_self_condemning', 'push_yta_social_proof'],\n",
    "    'presentation': ['firstperson_atfault', 'thirdperson'],\n",
    "    'robustness': ['add_extraneous_detail', 'change_trivial_detail', 'remove_sentence']\n",
    "}\n",
    "\n",
    "content_by_category = {}\n",
    "for cat_name, cat_types in categories.items():\n",
    "    cat_flips = content_flips[content_flips['perturbation_type'].isin(cat_types)]\n",
    "    cat_total = len(content_df[content_df['perturbation_type'].isin(cat_types)])\n",
    "    result = analyze_transitions(cat_flips, 'base_verdict', 'standardized_judgment', \n",
    "                                 f\"CONTENT - {cat_name.upper()} (flip rate: {len(cat_flips)/cat_total*100:.1f}%)\")\n",
    "    content_by_category[cat_name] = result\n",
    "    print_transition_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By individual perturbation type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY INDIVIDUAL PERTURBATION TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "perturbation_results = []\n",
    "for ptype in content_df['perturbation_type'].unique():\n",
    "    if ptype == 'none':\n",
    "        continue\n",
    "    \n",
    "    ptype_data = content_df[content_df['perturbation_type'] == ptype]\n",
    "    ptype_flips = content_flips[content_flips['perturbation_type'] == ptype]\n",
    "    \n",
    "    n_total = len(ptype_data)\n",
    "    n_flips = len(ptype_flips)\n",
    "    flip_rate = n_flips / n_total * 100 if n_total > 0 else 0\n",
    "    \n",
    "    if n_flips > 0:\n",
    "        ptype_flips = ptype_flips.copy()\n",
    "        ptype_flips['distance'] = ptype_flips.apply(\n",
    "            lambda r: get_transition_distance(r['base_verdict'], r['standardized_judgment']), axis=1\n",
    "        )\n",
    "        dist = ptype_flips['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    perturbation_results.append({\n",
    "        'perturbation': ptype,\n",
    "        'n_total': n_total,\n",
    "        'n_flips': n_flips,\n",
    "        'flip_rate': flip_rate,\n",
    "        'adjacent_pct': adj,\n",
    "        'medium_pct': med,\n",
    "        'opposite_pct': opp\n",
    "    })\n",
    "\n",
    "pert_df = pd.DataFrame(perturbation_results).sort_values('flip_rate', ascending=False)\n",
    "print(f\"\\n{'Perturbation Type':<30} {'Rows':>7} {'Flips':>6} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "print(\"-\" * 85)\n",
    "for _, row in pert_df.iterrows():\n",
    "    print(f\"{row['perturbation']:<30} {row['n_total']:>7} {row['n_flips']:>6} {row['flip_rate']:>5.1f}% | {row['adjacent_pct']:>5.1f}% {row['medium_pct']:>5.1f}% {row['opposite_pct']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY MODEL (Content Perturbations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_results = []\n",
    "for model in sorted(content_df['model'].unique()):\n",
    "    model_data = content_df[(content_df['model'] == model) & (content_df['perturbation_type'] != 'none')]\n",
    "    model_flips = content_flips[content_flips['model'] == model]\n",
    "    \n",
    "    n_total = len(model_data)\n",
    "    n_flips = len(model_flips)\n",
    "    flip_rate = n_flips / n_total * 100 if n_total > 0 else 0\n",
    "    \n",
    "    if n_flips > 0:\n",
    "        model_flips = model_flips.copy()\n",
    "        model_flips['distance'] = model_flips.apply(\n",
    "            lambda r: get_transition_distance(r['base_verdict'], r['standardized_judgment']), axis=1\n",
    "        )\n",
    "        dist = model_flips['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    model_results.append({\n",
    "        'model': model,\n",
    "        'n_flips': n_flips,\n",
    "        'flip_rate': flip_rate,\n",
    "        'adjacent_pct': adj,\n",
    "        'medium_pct': med,\n",
    "        'opposite_pct': opp\n",
    "    })\n",
    "\n",
    "model_df = pd.DataFrame(model_results)\n",
    "print(f\"\\n{'Model':<15} {'Flips':>7} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in model_df.iterrows():\n",
    "    print(f\"{row['model']:<15} {row['n_flips']:>7} {row['flip_rate']:>5.1f}% | {row['adjacent_pct']:>5.1f}% {row['medium_pct']:>5.1f}% {row['opposite_pct']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition asymmetry\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION ASYMMETRY (Content Perturbations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "content_asymmetry = compute_asymmetry(content_flips, 'base_verdict', 'standardized_judgment')\n",
    "print_asymmetry_table(content_asymmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition flow visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION FLOW: Where do verdicts go?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for base_v in VALID_VERDICTS:\n",
    "    base_flips = content_flips[content_flips['base_verdict'] == base_v]\n",
    "    n_base = len(base_flips)\n",
    "    \n",
    "    print(f\"\\n{base_v} ({SHORT_NAMES[base_v]}) → (n={n_base} flips)\")\n",
    "    for target_v in VALID_VERDICTS:\n",
    "        if target_v != base_v:\n",
    "            count = len(base_flips[base_flips['standardized_judgment'] == target_v])\n",
    "            pct = count / n_base * 100 if n_base > 0 else 0\n",
    "            bar = '█' * int(pct / 2)\n",
    "            print(f\"  → {SHORT_NAMES[target_v]:3} ({target_v:17}): {count:5} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Entropy Data Analysis\n",
    "\n",
    "Analysis of sampling variance (m=15 samples per scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entropy data\n",
    "entropy_df = pd.read_parquet(DATA_DIR / 'entropy_consolidated_m15.parquet')\n",
    "print(f\"Entropy data: {entropy_df.shape}\")\n",
    "print(f\"Models: {entropy_df['model'].unique()}\")\n",
    "print(f\"\\nColumns: {entropy_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid verdicts\n",
    "valid_entropy = entropy_df[\n",
    "    (entropy_df['master_run1_semantic'].isin(VALID_VERDICTS)) &\n",
    "    (entropy_df['majority_verdict_semantic'].isin(VALID_VERDICTS))\n",
    "].copy()\n",
    "\n",
    "# Identify mismatches (single run != majority)\n",
    "valid_entropy['mismatch'] = valid_entropy['master_run1_semantic'] != valid_entropy['majority_verdict_semantic']\n",
    "\n",
    "n_total = len(valid_entropy)\n",
    "n_mismatch = valid_entropy['mismatch'].sum()\n",
    "print(f\"Total valid scenarios: {n_total}\")\n",
    "print(f\"Mismatches (run1 ≠ majority): {n_mismatch} ({n_mismatch/n_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transitions for mismatches\n",
    "entropy_mismatches = valid_entropy[valid_entropy['mismatch'] == True].copy()\n",
    "\n",
    "entropy_result = analyze_transitions(entropy_mismatches, 'master_run1_semantic', 'majority_verdict_semantic',\n",
    "                                     f\"ENTROPY DATA - Run1 vs Majority (mismatch rate: {n_mismatch/n_total*100:.1f}%)\")\n",
    "print_transition_summary(entropy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY MODEL (Entropy - Run1 vs Majority)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "entropy_model_results = []\n",
    "for model in sorted(entropy_df['model'].unique()):\n",
    "    model_data = valid_entropy[valid_entropy['model'] == model]\n",
    "    model_mismatch = model_data[model_data['mismatch'] == True]\n",
    "    \n",
    "    n = len(model_data)\n",
    "    n_mis = len(model_mismatch)\n",
    "    rate = n_mis / n * 100 if n > 0 else 0\n",
    "    \n",
    "    if n_mis > 0:\n",
    "        model_mismatch = model_mismatch.copy()\n",
    "        model_mismatch['distance'] = model_mismatch.apply(\n",
    "            lambda r: get_transition_distance(r['master_run1_semantic'], r['majority_verdict_semantic']), axis=1\n",
    "        )\n",
    "        dist = model_mismatch['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    entropy_model_results.append({\n",
    "        'model': model,\n",
    "        'n_scenarios': n,\n",
    "        'n_mismatches': n_mis,\n",
    "        'mismatch_rate': rate,\n",
    "        'adjacent_pct': adj,\n",
    "        'medium_pct': med,\n",
    "        'opposite_pct': opp\n",
    "    })\n",
    "\n",
    "entropy_model_df = pd.DataFrame(entropy_model_results)\n",
    "print(f\"\\n{'Model':<15} {'Mismatch':>10} {'Rate':>7} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "print(\"-\" * 65)\n",
    "for _, row in entropy_model_df.iterrows():\n",
    "    print(f\"{row['model']:<15} {row['n_mismatches']:>3}/{row['n_scenarios']:<6} {row['mismatch_rate']:>5.1f}% | {row['adjacent_pct']:>5.1f}% {row['medium_pct']:>5.1f}% {row['opposite_pct']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-sample variance analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WITHIN-SAMPLE VARIANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_active_verdicts(counts_dict):\n",
    "    return [k for k, v in counts_dict.items() if v > 0 and k not in ['INFO']]\n",
    "\n",
    "def get_verdict_pattern(counts_dict):\n",
    "    active = get_active_verdicts(counts_dict)\n",
    "    if len(active) == 1:\n",
    "        return 'unanimous'\n",
    "    elif len(active) == 2:\n",
    "        return 'two_verdicts'\n",
    "    else:\n",
    "        return 'three_plus_verdicts'\n",
    "\n",
    "entropy_df['pattern'] = entropy_df['counts'].apply(get_verdict_pattern)\n",
    "\n",
    "print(\"\\nVerdict pattern distribution:\")\n",
    "pattern_counts = entropy_df['pattern'].value_counts()\n",
    "for p, c in pattern_counts.items():\n",
    "    print(f\"  {p:20}: {c:4} ({c/len(entropy_df)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\nPattern distribution by model:\")\n",
    "print(\"-\" * 70)\n",
    "for model in sorted(entropy_df['model'].unique()):\n",
    "    model_data = entropy_df[entropy_df['model'] == model]\n",
    "    n = len(model_data)\n",
    "    unanimous = (model_data['pattern'] == 'unanimous').sum()\n",
    "    two = (model_data['pattern'] == 'two_verdicts').sum()\n",
    "    three_plus = (model_data['pattern'] == 'three_plus_verdicts').sum()\n",
    "    print(f\"{model:12}: Unanimous {unanimous:3} ({unanimous/n*100:5.1f}%) | Two {two:3} ({two/n*100:5.1f}%) | 3+ {three_plus:3} ({three_plus/n*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze two-verdict pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TWO-VERDICT PAIR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_verdict_pair(counts_dict):\n",
    "    active = sorted([k for k, v in counts_dict.items() if v > 0 and k in ['YTA', 'NTA', 'ESH', 'NAH']])\n",
    "    if len(active) == 2:\n",
    "        return f\"{active[0]}-{active[1]}\"\n",
    "    return None\n",
    "\n",
    "def check_pair_adjacency(counts_dict):\n",
    "    active = [k for k, v in counts_dict.items() if v > 0 and k in ['YTA', 'NTA', 'ESH', 'NAH']]\n",
    "    if len(active) != 2:\n",
    "        return None\n",
    "    v1, v2 = AITA_TO_SEMANTIC.get(active[0]), AITA_TO_SEMANTIC.get(active[1])\n",
    "    if v1 and v2:\n",
    "        return get_transition_distance(v1, v2)\n",
    "    return None\n",
    "\n",
    "two_verdict = entropy_df[entropy_df['pattern'] == 'two_verdicts'].copy()\n",
    "two_verdict['pair'] = two_verdict['counts'].apply(get_verdict_pair)\n",
    "two_verdict['pair_type'] = two_verdict['counts'].apply(check_pair_adjacency)\n",
    "\n",
    "print(f\"\\nFor two-verdict cases (n={len(two_verdict)}):\")\n",
    "pair_types = two_verdict['pair_type'].value_counts()\n",
    "for p, c in pair_types.items():\n",
    "    if p:\n",
    "        print(f\"  {p:10}: {c:4} ({c/len(two_verdict)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\nMost common verdict pairs:\")\n",
    "pair_freq = two_verdict['pair'].value_counts()\n",
    "for pair, count in pair_freq.head(10).items():\n",
    "    if pair:\n",
    "        v1, v2 = pair.split('-')\n",
    "        s1, s2 = AITA_TO_SEMANTIC.get(v1), AITA_TO_SEMANTIC.get(v2)\n",
    "        if s1 and s2:\n",
    "            dist = get_transition_distance(s1, s2)\n",
    "            print(f\"  {pair:10}: {count:4} ({count/len(two_verdict)*100:5.1f}%)  [{dist}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Protocol Tests Analysis\n",
    "\n",
    "Analysis of verdict transitions under different prompt protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load protocol tests data\n",
    "protocol_df = pd.read_parquet(RESULTS_DIR / 'protocol_eval.parquet')\n",
    "print(f\"Protocol tests data: {protocol_df.shape}\")\n",
    "print(f\"Protocols: {protocol_df['protocol'].unique()}\")\n",
    "print(f\"Eval models: {protocol_df['eval_model'].unique()}\")\n",
    "\n",
    "# Standardize model names\n",
    "model_map = {\n",
    "    'qwen_2.5_72b_instruct': 'qwen25',\n",
    "    'deepseek_chat': 'deepseek',\n",
    "    'claude_3.7_sonnet': 'claude37',\n",
    "    'gpt_4.1': 'gpt41'\n",
    "}\n",
    "protocol_df['model'] = protocol_df['eval_model'].map(model_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid verdicts\n",
    "valid_protocol = protocol_df[\n",
    "    (protocol_df['main_study_verdict'].isin(VALID_VERDICTS)) &\n",
    "    (protocol_df['standardized_judgment'].isin(VALID_VERDICTS))\n",
    "].copy()\n",
    "\n",
    "valid_protocol['flipped'] = valid_protocol['main_study_verdict'] != valid_protocol['standardized_judgment']\n",
    "\n",
    "n_total = len(valid_protocol)\n",
    "n_flips = valid_protocol['flipped'].sum()\n",
    "print(f\"Valid rows: {n_total}\")\n",
    "print(f\"Total flips: {n_flips} ({n_flips/n_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall protocol transitions\n",
    "protocol_flips = valid_protocol[valid_protocol['flipped'] == True].copy()\n",
    "\n",
    "protocol_result = analyze_transitions(protocol_flips, 'main_study_verdict', 'standardized_judgment',\n",
    "                                      f\"PROTOCOL TESTS - ALL (flip rate: {n_flips/n_total*100:.1f}%)\")\n",
    "print_transition_summary(protocol_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By protocol type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY PROTOCOL TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "protocol_results = {}\n",
    "for proto in ['explanation_first', 'system_prompt', 'unstructured']:\n",
    "    proto_data = valid_protocol[valid_protocol['protocol'] == proto]\n",
    "    proto_flips = proto_data[proto_data['flipped'] == True]\n",
    "    \n",
    "    result = analyze_transitions(proto_flips, 'main_study_verdict', 'standardized_judgment',\n",
    "                                 f\"{proto.upper()} (flip rate: {len(proto_flips)/len(proto_data)*100:.1f}%)\")\n",
    "    protocol_results[proto] = result\n",
    "    print_transition_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY MODEL (Protocol Tests)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "proto_model_results = []\n",
    "for model in sorted(valid_protocol['model'].unique()):\n",
    "    model_data = valid_protocol[valid_protocol['model'] == model]\n",
    "    model_flips = model_data[model_data['flipped'] == True]\n",
    "    \n",
    "    n_total = len(model_data)\n",
    "    n_flips = len(model_flips)\n",
    "    \n",
    "    if n_flips > 0:\n",
    "        model_flips = model_flips.copy()\n",
    "        model_flips['distance'] = model_flips.apply(\n",
    "            lambda r: get_transition_distance(r['main_study_verdict'], r['standardized_judgment']), axis=1\n",
    "        )\n",
    "        dist = model_flips['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    proto_model_results.append({\n",
    "        'model': model,\n",
    "        'n_flips': n_flips,\n",
    "        'flip_rate': n_flips / n_total * 100,\n",
    "        'adjacent_pct': adj,\n",
    "        'medium_pct': med,\n",
    "        'opposite_pct': opp\n",
    "    })\n",
    "\n",
    "proto_model_df = pd.DataFrame(proto_model_results)\n",
    "print(f\"\\n{'Model':<15} {'Flips':>7} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "print(\"-\" * 60)\n",
    "for _, row in proto_model_df.iterrows():\n",
    "    print(f\"{row['model']:<15} {row['n_flips']:>7} {row['flip_rate']:>5.1f}% | {row['adjacent_pct']:>5.1f}% {row['medium_pct']:>5.1f}% {row['opposite_pct']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model × Protocol\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL × PROTOCOL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for proto in ['explanation_first', 'system_prompt', 'unstructured']:\n",
    "    print(f\"\\n{proto.upper()}:\")\n",
    "    print(f\"{'Model':12} {'Flips':>6} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    proto_data = valid_protocol[valid_protocol['protocol'] == proto]\n",
    "    \n",
    "    for model in sorted(valid_protocol['model'].unique()):\n",
    "        model_data = proto_data[proto_data['model'] == model]\n",
    "        model_flips = model_data[model_data['flipped'] == True]\n",
    "        \n",
    "        n_total = len(model_data)\n",
    "        n_flips = len(model_flips)\n",
    "        \n",
    "        if n_flips > 0:\n",
    "            model_flips = model_flips.copy()\n",
    "            model_flips['distance'] = model_flips.apply(\n",
    "                lambda r: get_transition_distance(r['main_study_verdict'], r['standardized_judgment']), axis=1\n",
    "            )\n",
    "            dist = model_flips['distance'].value_counts(normalize=True) * 100\n",
    "            adj = dist.get('adjacent', 0)\n",
    "            med = dist.get('medium', 0)\n",
    "            opp = dist.get('opposite', 0)\n",
    "        else:\n",
    "            adj, med, opp = 0, 0, 0\n",
    "        \n",
    "        print(f\"{model:12} {n_flips:>6} {n_flips/n_total*100:>5.1f}% | {adj:>5.1f}% {med:>5.1f}% {opp:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol transition asymmetry\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION ASYMMETRY (Protocol Tests)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "protocol_asymmetry = compute_asymmetry(protocol_flips, 'main_study_verdict', 'standardized_judgment')\n",
    "print_asymmetry_table(protocol_asymmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol transition flow\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION FLOW (Protocol Tests): Where do verdicts go?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for base_v in VALID_VERDICTS:\n",
    "    base_flips = protocol_flips[protocol_flips['main_study_verdict'] == base_v]\n",
    "    n_base = len(base_flips)\n",
    "    \n",
    "    print(f\"\\n{base_v} ({SHORT_NAMES[base_v]}) → (n={n_base} flips)\")\n",
    "    for target_v in VALID_VERDICTS:\n",
    "        if target_v != base_v:\n",
    "            count = len(base_flips[base_flips['standardized_judgment'] == target_v])\n",
    "            pct = count / n_base * 100 if n_base > 0 else 0\n",
    "            bar = '█' * int(pct / 2)\n",
    "            print(f\"  → {SHORT_NAMES[target_v]:3} ({target_v:17}): {count:5} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Reasoning Models Analysis\n",
    "\n",
    "Analysis of verdict transitions for reasoning models (o3-mini, claude-thinking, deepseek-r1, qwq-32b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reasoning models data\n",
    "reasoning_df = pd.read_parquet(DATA_DIR / 'reasoning_eval.parquet')\n",
    "print(f\"Reasoning models data: {reasoning_df.shape}\")\n",
    "print(f\"Models: {reasoning_df['model'].unique()}\")\n",
    "print(f\"Protocols: {reasoning_df['protocol'].unique()}\")\n",
    "print(f\"Perturbation types: {reasoning_df['perturbation_type'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to valid verdicts and flips\n",
    "valid_reasoning = reasoning_df[\n",
    "    (reasoning_df['base_verdict'].isin(VALID_VERDICTS)) &\n",
    "    (reasoning_df['standardized_judgment'].isin(VALID_VERDICTS))\n",
    "].copy()\n",
    "\n",
    "# Check if verdict_flipped exists or compute it\n",
    "if 'verdict_flipped' not in valid_reasoning.columns:\n",
    "    valid_reasoning['verdict_flipped'] = valid_reasoning['base_verdict'] != valid_reasoning['standardized_judgment']\n",
    "\n",
    "n_total = len(valid_reasoning)\n",
    "n_flips = valid_reasoning['verdict_flipped'].sum()\n",
    "print(f\"Valid rows: {n_total}\")\n",
    "print(f\"Total flips: {n_flips} ({n_flips/n_total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall reasoning model transitions\n",
    "reasoning_flips = valid_reasoning[valid_reasoning['verdict_flipped'] == True].copy()\n",
    "\n",
    "reasoning_result = analyze_transitions(reasoning_flips, 'base_verdict', 'standardized_judgment',\n",
    "                                       f\"REASONING MODELS - ALL (flip rate: {n_flips/n_total*100:.1f}%)\")\n",
    "print_transition_summary(reasoning_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By reasoning model\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY REASONING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reasoning_model_results = []\n",
    "for model in sorted(reasoning_df['model'].unique()):\n",
    "    model_data = valid_reasoning[valid_reasoning['model'] == model]\n",
    "    model_flips = model_data[model_data['verdict_flipped'] == True]\n",
    "    \n",
    "    n_total = len(model_data)\n",
    "    n_flips = len(model_flips)\n",
    "    \n",
    "    if n_flips > 0:\n",
    "        model_flips = model_flips.copy()\n",
    "        model_flips['distance'] = model_flips.apply(\n",
    "            lambda r: get_transition_distance(r['base_verdict'], r['standardized_judgment']), axis=1\n",
    "        )\n",
    "        dist = model_flips['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    reasoning_model_results.append({\n",
    "        'model': model,\n",
    "        'n_total': n_total,\n",
    "        'n_flips': n_flips,\n",
    "        'flip_rate': n_flips / n_total * 100 if n_total > 0 else 0,\n",
    "        'adjacent_pct': adj,\n",
    "        'medium_pct': med,\n",
    "        'opposite_pct': opp\n",
    "    })\n",
    "\n",
    "reasoning_model_df = pd.DataFrame(reasoning_model_results)\n",
    "print(f\"\\n{'Model':<20} {'Total':>7} {'Flips':>6} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "print(\"-\" * 75)\n",
    "for _, row in reasoning_model_df.iterrows():\n",
    "    print(f\"{row['model']:<20} {row['n_total']:>7} {row['n_flips']:>6} {row['flip_rate']:>5.1f}% | {row['adjacent_pct']:>5.1f}% {row['medium_pct']:>5.1f}% {row['opposite_pct']:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By protocol for reasoning models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BY PROTOCOL (Reasoning Models)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for proto in reasoning_df['protocol'].unique():\n",
    "    proto_data = valid_reasoning[valid_reasoning['protocol'] == proto]\n",
    "    proto_flips = proto_data[proto_data['verdict_flipped'] == True]\n",
    "    \n",
    "    n_total = len(proto_data)\n",
    "    n_flips = len(proto_flips)\n",
    "    \n",
    "    if n_flips > 0:\n",
    "        proto_flips = proto_flips.copy()\n",
    "        proto_flips['distance'] = proto_flips.apply(\n",
    "            lambda r: get_transition_distance(r['base_verdict'], r['standardized_judgment']), axis=1\n",
    "        )\n",
    "        dist = proto_flips['distance'].value_counts(normalize=True) * 100\n",
    "        adj = dist.get('adjacent', 0)\n",
    "        med = dist.get('medium', 0)\n",
    "        opp = dist.get('opposite', 0)\n",
    "    else:\n",
    "        adj, med, opp = 0, 0, 0\n",
    "    \n",
    "    print(f\"\\n{proto.upper()}: {n_flips}/{n_total} flips ({n_flips/n_total*100:.1f}%)\")\n",
    "    print(f\"  Adjacent: {adj:5.1f}%  Medium: {med:5.1f}%  Opposite: {opp:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model × Protocol for reasoning models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL × PROTOCOL (Reasoning Models)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for proto in reasoning_df['protocol'].unique():\n",
    "    print(f\"\\n{proto.upper()}:\")\n",
    "    print(f\"{'Model':<20} {'Flips':>6} {'Rate':>6} | {'Adj%':>6} {'Med%':>6} {'Opp%':>6}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    proto_data = valid_reasoning[valid_reasoning['protocol'] == proto]\n",
    "    \n",
    "    for model in sorted(reasoning_df['model'].unique()):\n",
    "        model_data = proto_data[proto_data['model'] == model]\n",
    "        model_flips = model_data[model_data['verdict_flipped'] == True]\n",
    "        \n",
    "        n_total = len(model_data)\n",
    "        n_flips = len(model_flips)\n",
    "        \n",
    "        if n_total == 0:\n",
    "            continue\n",
    "        \n",
    "        if n_flips > 0:\n",
    "            model_flips = model_flips.copy()\n",
    "            model_flips['distance'] = model_flips.apply(\n",
    "                lambda r: get_transition_distance(r['base_verdict'], r['standardized_judgment']), axis=1\n",
    "            )\n",
    "            dist = model_flips['distance'].value_counts(normalize=True) * 100\n",
    "            adj = dist.get('adjacent', 0)\n",
    "            med = dist.get('medium', 0)\n",
    "            opp = dist.get('opposite', 0)\n",
    "        else:\n",
    "            adj, med, opp = 0, 0, 0\n",
    "        \n",
    "        print(f\"{model:<20} {n_flips:>6} {n_flips/n_total*100:>5.1f}% | {adj:>5.1f}% {med:>5.1f}% {opp:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning models transition asymmetry\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION ASYMMETRY (Reasoning Models)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(reasoning_flips) > 0:\n",
    "    reasoning_asymmetry = compute_asymmetry(reasoning_flips, 'base_verdict', 'standardized_judgment')\n",
    "    print_asymmetry_table(reasoning_asymmetry)\n",
    "else:\n",
    "    print(\"No flips found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning models transition flow\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSITION FLOW (Reasoning Models): Where do verdicts go?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for base_v in VALID_VERDICTS:\n",
    "    base_flips = reasoning_flips[reasoning_flips['base_verdict'] == base_v]\n",
    "    n_base = len(base_flips)\n",
    "    \n",
    "    if n_base == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{base_v} ({SHORT_NAMES[base_v]}) → (n={n_base} flips)\")\n",
    "    for target_v in VALID_VERDICTS:\n",
    "        if target_v != base_v:\n",
    "            count = len(base_flips[base_flips['standardized_judgment'] == target_v])\n",
    "            pct = count / n_base * 100 if n_base > 0 else 0\n",
    "            bar = '█' * int(pct / 2)\n",
    "            print(f\"  → {SHORT_NAMES[target_v]:3} ({target_v:17}): {count:5} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comprehensive Comparison\n",
    "\n",
    "Summary comparison across all four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compile summary statistics using OP status framework\nsummary_data = []\n\n# Content perturbations\nif content_result:\n    summary_data.append({\n        'Dataset': 'Content Perturbations',\n        'N Flips': content_result['n_flips'],\n        'Preserved %': content_result['preserved_pct'],\n        'Reversed %': content_result['reversed_pct'],\n        'Net Toward Blame': content_result['net_toward_blame']\n    })\n\n# Entropy data\nif entropy_result:\n    summary_data.append({\n        'Dataset': 'Entropy (run1 vs majority)',\n        'N Flips': entropy_result['n_flips'],\n        'Preserved %': entropy_result['preserved_pct'],\n        'Reversed %': entropy_result['reversed_pct'],\n        'Net Toward Blame': entropy_result['net_toward_blame']\n    })\n\n# Protocol tests\nif protocol_result:\n    summary_data.append({\n        'Dataset': 'Protocol Tests',\n        'N Flips': protocol_result['n_flips'],\n        'Preserved %': protocol_result['preserved_pct'],\n        'Reversed %': protocol_result['reversed_pct'],\n        'Net Toward Blame': protocol_result['net_toward_blame']\n    })\n\n# Reasoning models\nif reasoning_result:\n    summary_data.append({\n        'Dataset': 'Reasoning Models',\n        'N Flips': reasoning_result['n_flips'],\n        'Preserved %': reasoning_result['preserved_pct'],\n        'Reversed %': reasoning_result['reversed_pct'],\n        'Net Toward Blame': reasoning_result['net_toward_blame']\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nprint(\"=\"*80)\nprint(\"SUMMARY COMPARISON (OP Blame Status Framework)\")\nprint(\"=\"*80)\nprint(\"\\nOP Status Preserved = transition within {YTA,ESH} or {NTA,NAH}\")\nprint(\"OP Status Reversed = transition crosses blame-status boundary\\n\")\nprint(summary_df.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization: OP status by dataset\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Preserved vs Reversed\nax = axes[0]\nx = range(len(summary_df))\nwidth = 0.35\n\nbars1 = ax.bar([i - width/2 for i in x], summary_df['Preserved %'], width, \n               label='Status Preserved', color='#2ecc71')\nbars2 = ax.bar([i + width/2 for i in x], summary_df['Reversed %'], width, \n               label='Status Reversed', color='#e74c3c')\n\nax.set_ylabel('Percentage of Flips')\nax.set_title('A. OP Blame Status Preservation')\nax.set_xticks(x)\nax.set_xticklabels(summary_df['Dataset'], rotation=15, ha='right')\nax.legend()\nax.set_ylim(0, 80)\nax.axhline(50, color='gray', linestyle='--', alpha=0.5, label='50%')\n\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax.annotate(f'{height:.1f}%',\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', fontsize=9)\n\n# Panel B: Net flow toward blame\nax = axes[1]\ncolors = ['#e74c3c' if x > 0 else '#2ecc71' for x in summary_df['Net Toward Blame']]\nbars = ax.barh(range(len(summary_df)), summary_df['Net Toward Blame'], color=colors)\nax.set_yticks(range(len(summary_df)))\nax.set_yticklabels(summary_df['Dataset'])\nax.axvline(0, color='black', linewidth=0.5)\nax.set_xlabel('Net Transitions (+ = toward blame, - = toward exoneration)')\nax.set_title('B. Net Direction of Verdict Changes')\n\nfor i, (bar, val) in enumerate(zip(bars, summary_df['Net Toward Blame'])):\n    ax.annotate(f'{val:+d}',\n                xy=(val, i),\n                xytext=(5 if val >= 0 else -5, 0),\n                textcoords=\"offset points\",\n                ha='left' if val >= 0 else 'right', va='center', fontsize=10)\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / 'verdict_transitions_op_status.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Print key findings\nprint(\"=\"*80)\nprint(\"KEY FINDINGS (OP Blame Status Framework)\")\nprint(\"=\"*80)\n\nprint(\"\"\"\nOP BLAME STATUS FRAMEWORK:\n==========================\n\nNarrator-implicated (OP blamed): Self_At_Fault (YTA), All_At_Fault (ESH)\nNarrator-exonerated (OP not blamed): Other_At_Fault (NTA), No_One_At_Fault (NAH)\n\nSTATUS PRESERVED: Transition stays within same blame-status group\n  - YTA <-> ESH (both implicate OP)\n  - NTA <-> NAH (both exonerate OP)\n  \nSTATUS REVERSED: Transition crosses blame-status boundary\n  - Any transition between {YTA,ESH} and {NTA,NAH}\n  - These are complete culpability reversals\n\nKEY OBSERVATIONS:\n\n1. CONTENT PERTURBATIONS show ~58% status-reversed transitions - a substantial\n   rate of complete culpability reversals that cannot be attributed to \n   fine-grained boundary uncertainty.\n\n2. DIRECTIONAL ASYMMETRY by perturbation type:\n   - push_yta: WORKS AS INTENDED (net flow toward blame)\n   - push_nta: BACKFIRES! (self-justification increases blame)\n   - presentation: Variable, third-person shifts toward blame\n   - robustness: Near-balanced (functions as noise)\n\n3. PROTOCOL TESTS show SYSTEMATIC EXONERATION:\n   - All protocol changes reduce narrator blame\n   - Unstructured shows largest exoneration bias\n   - The \"moral judge\" persona is constructed by forced-choice scaffolding\n\n4. MODEL DIFFERENCES:\n   - GPT-4.1: Most status-preserved (72%), coherent transitions\n   - Qwen-2.5: Mostly status-reversed (78%), binary verdict style\n   \n5. TWO DISTINCT MECHANISMS:\n   - Content perturbations operate through CREDIBILITY HEURISTICS\n     (self-blame trusted, self-justification triggers skepticism)\n   - Protocol perturbations operate through SCAFFOLDING\n     (forced-choice elicits judgment, open-ended elicits advice)\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare asymmetry patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ASYMMETRY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nCONTENT PERTURBATIONS:\")\n",
    "print_asymmetry_table(content_asymmetry)\n",
    "\n",
    "print(\"\\nPROTOCOL TESTS:\")\n",
    "print_asymmetry_table(protocol_asymmetry)\n",
    "\n",
    "if len(reasoning_flips) > 0:\n",
    "    print(\"\\nREASONING MODELS:\")\n",
    "    print_asymmetry_table(reasoning_asymmetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 6. Directional Analysis by Perturbation Type\n\nThis section analyzes the direction of OP blame status changes, revealing systematic patterns that vary by perturbation type.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Analyze OP direction by perturbation category (Content)\nprint(\"=\"*90)\nprint(\"CONTENT PERTURBATIONS: OP DIRECTION BY CATEGORY\")\nprint(\"=\"*90)\n\n# Define detailed perturbation categories\ndetailed_categories = {\n    'push_yta': ['push_yta_pattern_admission', 'push_yta_self_condemning', 'push_yta_social_proof'],\n    'push_nta': ['push_nta_self_justifying', 'push_nta_social_proof', 'push_nta_victim_pattern'],\n    'presentation': ['firstperson_atfault', 'thirdperson'],\n    'robustness': ['add_extraneous_detail', 'change_trivial_detail', 'remove_sentence']\n}\n\nfor cat_name, cat_types in detailed_categories.items():\n    cat_flips = content_flips[content_flips['perturbation_type'].isin(cat_types)].copy()\n    \n    if len(cat_flips) == 0:\n        continue\n        \n    cat_flips['op_direction'] = cat_flips.apply(\n        lambda r: get_op_direction(r['base_verdict'], r['standardized_judgment']), axis=1\n    )\n    \n    dir_counts = cat_flips['op_direction'].value_counts()\n    newly_blamed = dir_counts.get('OP_newly_blamed', 0)\n    exonerated = dir_counts.get('OP_exonerated', 0)\n    net = newly_blamed - exonerated\n    \n    print(f\"\\n{cat_name.upper()} (n={len(cat_flips)} flips)\")\n    print(f\"  OP newly blamed:     {newly_blamed:5}\")\n    print(f\"  OP exonerated:       {exonerated:5}\")\n    print(f\"  OP stays blamed:     {dir_counts.get('OP_stays_blamed', 0):5}\")\n    print(f\"  OP stays exonerated: {dir_counts.get('OP_stays_exonerated', 0):5}\")\n    print(f\"  NET: {net:+5} ({'toward blame' if net > 0 else 'toward exoneration'})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Detailed breakdown by individual perturbation type\nprint(\"\\n\" + \"=\"*90)\nprint(\"INDIVIDUAL PERTURBATION TYPE: OP DIRECTION ANALYSIS\")\nprint(\"=\"*90)\n\nindividual_results = compute_op_direction_by_category(\n    content_flips, 'base_verdict', 'standardized_judgment', 'perturbation_type'\n)\n\nprint(f\"\\n{'Perturbation':<35} {'Flips':>6} {'→Blame':>7} {'→Exon':>7} {'Net':>7} {'Direction':<18}\")\nprint(\"-\" * 90)\nfor _, row in individual_results.sort_values('net', ascending=False).iterrows():\n    print(f\"{row['category']:<35} {row['n_flips']:>6} {row['newly_blamed']:>7} {row['exonerated']:>7} {row['net']:>+7} {row['direction']:<18}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Protocol Tests: OP direction by protocol\nprint(\"\\n\" + \"=\"*90)\nprint(\"PROTOCOL TESTS: OP DIRECTION BY PROTOCOL\")\nprint(\"=\"*90)\n\nfor proto in ['explanation_first', 'system_prompt', 'unstructured']:\n    proto_flips = protocol_flips[protocol_flips['protocol'] == proto].copy()\n    \n    if len(proto_flips) == 0:\n        continue\n    \n    proto_flips['op_direction'] = proto_flips.apply(\n        lambda r: get_op_direction(r['main_study_verdict'], r['standardized_judgment']), axis=1\n    )\n    \n    dir_counts = proto_flips['op_direction'].value_counts()\n    newly_blamed = dir_counts.get('OP_newly_blamed', 0)\n    exonerated = dir_counts.get('OP_exonerated', 0)\n    net = newly_blamed - exonerated\n    \n    ratio = newly_blamed / exonerated if exonerated > 0 else float('inf')\n    ratio_str = f\"{ratio:.1f}x toward blame\" if ratio > 1 else f\"{1/ratio:.1f}x toward exoneration\" if ratio < 1 else \"balanced\"\n    \n    print(f\"\\n{proto.upper()} (n={len(proto_flips)} flips)\")\n    print(f\"  OP newly blamed:     {newly_blamed:5}\")\n    print(f\"  OP exonerated:       {exonerated:5}\")\n    print(f\"  OP stays blamed:     {dir_counts.get('OP_stays_blamed', 0):5}\")\n    print(f\"  OP stays exonerated: {dir_counts.get('OP_stays_exonerated', 0):5}\")\n    print(f\"  NET: {net:+5}  ({ratio_str})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Reasoning Models: OP direction by model\nprint(\"\\n\" + \"=\"*90)\nprint(\"REASONING MODELS: OP DIRECTION BY MODEL\")\nprint(\"=\"*90)\n\nfor model in sorted(reasoning_df['model'].unique()):\n    model_flips = reasoning_flips[reasoning_flips['model'] == model].copy()\n    if len(model_flips) == 0:\n        continue\n    \n    model_flips['op_direction'] = model_flips.apply(\n        lambda r: get_op_direction(r['base_verdict'], r['standardized_judgment']), axis=1\n    )\n    \n    dir_counts = model_flips['op_direction'].value_counts()\n    newly_blamed = dir_counts.get('OP_newly_blamed', 0)\n    exonerated = dir_counts.get('OP_exonerated', 0)\n    net = newly_blamed - exonerated\n    \n    print(f\"\\n{model} (n={len(model_flips)} flips)\")\n    print(f\"  OP newly blamed:     {newly_blamed:5}\")\n    print(f\"  OP exonerated:       {exonerated:5}\")\n    print(f\"  OP stays blamed:     {dir_counts.get('OP_stays_blamed', 0):5}\")\n    print(f\"  OP stays exonerated: {dir_counts.get('OP_stays_exonerated', 0):5}\")\n    print(f\"  NET: {net:+5} ({'toward blame' if net > 0 else 'toward exoneration' if net < 0 else 'balanced'})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization: OP direction comparison\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Content perturbations by category\nax = axes[0]\ncat_results = []\nfor cat_name, cat_types in detailed_categories.items():\n    cat_flips = content_flips[content_flips['perturbation_type'].isin(cat_types)].copy()\n    if len(cat_flips) == 0:\n        continue\n    cat_flips['op_direction'] = cat_flips.apply(\n        lambda r: get_op_direction(r['base_verdict'], r['standardized_judgment']), axis=1\n    )\n    dir_counts = cat_flips['op_direction'].value_counts()\n    cat_results.append({\n        'category': cat_name,\n        'newly_blamed': dir_counts.get('OP_newly_blamed', 0),\n        'exonerated': dir_counts.get('OP_exonerated', 0)\n    })\ncat_df = pd.DataFrame(cat_results)\n\nx = range(len(cat_df))\nwidth = 0.35\nax.bar([i - width/2 for i in x], cat_df['newly_blamed'], width, label='OP Newly Blamed', color='#e74c3c')\nax.bar([i + width/2 for i in x], cat_df['exonerated'], width, label='OP Exonerated', color='#2ecc71')\nax.set_xticks(x)\nax.set_xticklabels(cat_df['category'], rotation=15, ha='right')\nax.set_ylabel('Count')\nax.set_title('A. Content Perturbations')\nax.legend()\n\n# Protocol tests\nax = axes[1]\nproto_results = []\nfor proto in ['explanation_first', 'system_prompt', 'unstructured']:\n    proto_flips = protocol_flips[protocol_flips['protocol'] == proto].copy()\n    if len(proto_flips) == 0:\n        continue\n    proto_flips['op_direction'] = proto_flips.apply(\n        lambda r: get_op_direction(r['main_study_verdict'], r['standardized_judgment']), axis=1\n    )\n    dir_counts = proto_flips['op_direction'].value_counts()\n    proto_results.append({\n        'protocol': proto.replace('_', '\\n'),\n        'newly_blamed': dir_counts.get('OP_newly_blamed', 0),\n        'exonerated': dir_counts.get('OP_exonerated', 0)\n    })\nproto_df = pd.DataFrame(proto_results)\n\nx = range(len(proto_df))\nax.bar([i - width/2 for i in x], proto_df['newly_blamed'], width, label='OP Newly Blamed', color='#e74c3c')\nax.bar([i + width/2 for i in x], proto_df['exonerated'], width, label='OP Exonerated', color='#2ecc71')\nax.set_xticks(x)\nax.set_xticklabels(proto_df['protocol'])\nax.set_ylabel('Count')\nax.set_title('B. Protocol Tests')\nax.legend()\n\n# Net direction comparison\nax = axes[2]\nall_results = []\nfor cat_name, cat_types in detailed_categories.items():\n    cat_flips = content_flips[content_flips['perturbation_type'].isin(cat_types)].copy()\n    if len(cat_flips) == 0:\n        continue\n    cat_flips['op_direction'] = cat_flips.apply(\n        lambda r: get_op_direction(r['base_verdict'], r['standardized_judgment']), axis=1\n    )\n    dir_counts = cat_flips['op_direction'].value_counts()\n    net = dir_counts.get('OP_newly_blamed', 0) - dir_counts.get('OP_exonerated', 0)\n    all_results.append({'source': f'C: {cat_name}', 'net': net})\n\nfor proto in ['explanation_first', 'system_prompt', 'unstructured']:\n    proto_flips = protocol_flips[protocol_flips['protocol'] == proto].copy()\n    if len(proto_flips) == 0:\n        continue\n    proto_flips['op_direction'] = proto_flips.apply(\n        lambda r: get_op_direction(r['main_study_verdict'], r['standardized_judgment']), axis=1\n    )\n    dir_counts = proto_flips['op_direction'].value_counts()\n    net = dir_counts.get('OP_newly_blamed', 0) - dir_counts.get('OP_exonerated', 0)\n    all_results.append({'source': f'P: {proto[:8]}', 'net': net})\n\nall_df = pd.DataFrame(all_results)\ncolors = ['#e74c3c' if x > 0 else '#2ecc71' for x in all_df['net']]\nax.barh(range(len(all_df)), all_df['net'], color=colors)\nax.set_yticks(range(len(all_df)))\nax.set_yticklabels(all_df['source'])\nax.axvline(0, color='black', linewidth=0.5)\nax.set_xlabel('Net (+ = toward blame)')\nax.set_title('C. Net Direction')\n\nplt.tight_layout()\nplt.savefig(FIGURES_DIR / 'verdict_transitions_op_direction.png', dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Summary: Key Insights from Directional Analysis\nprint(\"=\"*90)\nprint(\"KEY INSIGHTS: DIRECTIONAL PATTERNS IN VERDICT TRANSITIONS\")\nprint(\"=\"*90)\n\nprint(\"\"\"\n1. CONTENT PERTURBATIONS: CREDIBILITY HEURISTIC\n   ============================================\n   \n   Category          | Net Direction    | Interpretation\n   ------------------|------------------|--------------------------------------------\n   push_yta          | → TOWARD BLAME   | WORKS AS INTENDED (self-blame cues trusted)\n   push_nta          | → TOWARD BLAME   | BACKFIRES! (self-justification increases blame)\n   presentation      | → TOWARD BLAME   | Third-person depersonalization harshens judgment\n   robustness        | → TOWARD BLAME   | Slight bias (should be balanced)\n   \n   KEY FINDING: Push-NTA perturbations BACKFIRE!\n   - self_justifying: Net +191 toward blame instead of exoneration\n   - social_proof: Barely effective\n   - victim_pattern: Works somewhat\n   \n   This supports the \"credibility heuristic\" interpretation: self-criticism \n   is trusted, self-justification triggers skepticism.\n\n2. PROTOCOL PERTURBATIONS: SCAFFOLDING EFFECT\n   ==========================================\n   \n   Protocol          | Net Direction         | Interpretation\n   ------------------|----------------------|----------------------------------\n   explanation_first | → TOWARD EXONERATION | Deliberation softens judgment\n   system_prompt     | → TOWARD EXONERATION | Structure change reduces blame\n   unstructured      | → TOWARD EXONERATION | Massive exoneration bias\n   \n   KEY FINDING: Protocol changes SYSTEMATICALLY reduce narrator blame.\n   - The unstructured protocol almost NEVER increases blame\n   - This is NOT random noise - it's a directional bias\n   - Removing judgment scaffolding = exonerating the narrator\n\n3. THEORETICAL IMPLICATIONS\n   ========================\n   \n   TWO DISTINCT MECHANISMS:\n   \n   A) Content perturbations operate through CREDIBILITY HEURISTICS:\n      - Self-blame cues are trusted → blame increases (intended)\n      - Self-justification backfires → blame increases (unintended)\n      - Third-person narration provides \"distance\" → blame increases\n   \n   B) Protocol perturbations operate through SCAFFOLDING:\n      - Forced-choice elicits judgment → more blame\n      - Open-ended elicits advice/support → less blame\n      - The \"moral judge\" persona is CONSTRUCTED by the protocol\n\n4. IMPLICATIONS FOR PAPER FRAMING\n   ================================\n   \n   The ~58% status-reversed transition rate is NOT evidence of random \n   incoherence - it's evidence of SYSTEMATIC DIRECTIONAL BIAS that varies\n   by perturbation type:\n   \n   - Content: Credibility heuristics (self-justification backfires)\n   - Protocol: Scaffolding effects (removing forced-choice exonerates)\n   \n   This reframes \"instability\" as \"predictable directional sensitivity\"\n   with different mechanisms for different perturbation families.\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}