{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Combined Figure: Epistemic Stance + Cross-Model Agreement\n\nTwo-panel figure:\n- **(A)** Epistemic Stance Change by Perturbation (horizontal bar chart with 95% CIs)\n- **(B)** Cross-Model Agreement by Scenario Verification Level"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom scipy.stats import norm, pointbiserialr\nfrom pathlib import Path\nimport re\n\n%matplotlib inline"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication settings\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 9,\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "})\n",
    "\n",
    "z = norm.ppf(0.975)  # 1.96 for 95% CI\n",
    "\n",
    "def calculate_ci(p, n):\n",
    "    \"\"\"Calculate 95% CI for proportion.\"\"\"\n",
    "    if n == 0 or p == 0 or p == 1:\n",
    "        return 0\n",
    "    se = np.sqrt(p * (1 - p) / n)\n",
    "    return z * se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Part A: Epistemic Stance Data (Bar Chart)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epistemic stance lexicon\n",
    "EPISTEMIC_HEDGES = [\n",
    "    'seem*', 'appear*', 'might', 'could', 'may',\n",
    "    'perhaps', 'possibly', 'maybe', 'probably', 'likely',\n",
    "    'unclear', 'uncertain', 'unsure', 'guess*',\n",
    "]\n",
    "\n",
    "EPISTEMIC_BOOSTERS = [\n",
    "    'clearly', 'obviously', 'definitely', 'certainly',\n",
    "    'undoubtedly', 'unquestionably', 'absolutely',\n",
    "    'always', 'never', 'sure',\n",
    "]\n",
    "\n",
    "NEGATION_WORDS = ['not', 'no', \"n't\", 'never', 'neither', 'nor']\n",
    "\n",
    "def term_to_pattern(term: str) -> str:\n",
    "    term = term.lower()\n",
    "    if term.endswith('*'):\n",
    "        return r'\\b' + re.escape(term[:-1]) + r'\\w*\\b'\n",
    "    return r'\\b' + re.escape(term) + r'\\b'\n",
    "\n",
    "def is_negated(text: str, match_start: int, window: int = 3) -> bool:\n",
    "    before = text[:match_start].lower().split()[-window:]\n",
    "    return any(w in NEGATION_WORDS or \"n't\" in w for w in before)\n",
    "\n",
    "def count_markers(text: str, markers: list) -> int:\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = text.lower()\n",
    "    count = 0\n",
    "    for term in markers:\n",
    "        pattern = term_to_pattern(term)\n",
    "        try:\n",
    "            for match in re.finditer(pattern, text_lower):\n",
    "                if not is_negated(text_lower, match.start()):\n",
    "                    count += 1\n",
    "        except re.error:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def compute_net_epistemic(text: str) -> float:\n",
    "    if not text or pd.isna(text):\n",
    "        return 0.0\n",
    "    words = len(text.split())\n",
    "    if words == 0:\n",
    "        return 0.0\n",
    "    hedges = count_markers(text, EPISTEMIC_HEDGES)\n",
    "    boosters = count_markers(text, EPISTEMIC_BOOSTERS)\n",
    "    return ((boosters - hedges) / words) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading master parquet...\n",
      "Master data: 164,424 rows\n",
      "Models: ['claude37' 'gpt41' 'qwen25' 'deepseek']\n"
     ]
    }
   ],
   "source": [
    "# Load master data\n",
    "print(\"Loading master parquet...\")\n",
    "df = pd.read_parquet('../data/content_eval.parquet')\n",
    "print(f\"Master data: {len(df):,} rows\")\n",
    "print(f\"Models: {df['model'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing epistemic stance...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Compute epistemic stance\n",
    "print(\"Computing epistemic stance...\")\n",
    "df['net_epistemic'] = df['explanation'].apply(compute_net_epistemic)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Perturbation config\nPERTURBATION_CONFIG = {\n    'push_yta_social_proof': {'name': 'Social proof (against)', 'category': 'Persuasion'},\n    'push_yta_pattern_admission': {'name': 'Pattern admission', 'category': 'Persuasion'},\n    'push_yta_self_condemning': {'name': 'Self-condemning', 'category': 'Persuasion'},\n    'change_trivial_detail': {'name': 'Change trivial detail', 'category': 'Surface'},\n    'add_extraneous_detail': {'name': 'Add extraneous detail', 'category': 'Surface'},\n    'remove_sentence': {'name': 'Remove sentence', 'category': 'Surface'},\n    'push_nta_victim_pattern': {'name': 'Victim pattern', 'category': 'Persuasion'},\n    'push_nta_self_justifying': {'name': 'Self-justifying', 'category': 'Persuasion'},\n    'push_nta_social_proof': {'name': 'Social proof (for)', 'category': 'Persuasion'},\n    'firstperson_atfault': {'name': 'First-person', 'category': 'Point-of-view'},\n    'thirdperson': {'name': 'Third-person', 'category': 'Point-of-view'},\n}\n\n# Get baseline epistemic stance\nbaseline = df[df['perturbation_type'] == 'none'].copy()\nbaseline_rates = baseline.groupby(['id', 'model', 'run_number']).agg({\n    'net_epistemic': 'first'\n}).reset_index()\nbaseline_rates.columns = ['id', 'model', 'run_number', 'base_epistemic']\n\n# Get perturbation data and merge\nperturbations = df[df['perturbation_type'].isin(PERTURBATION_CONFIG.keys())].copy()\nmerged = perturbations.merge(baseline_rates, on=['id', 'model', 'run_number'], how='inner')\nmerged['net_delta'] = merged['net_epistemic'] - merged['base_epistemic']\nprint(f\"Matched pairs: {len(merged):,}\")\n\n# Aggregate by perturbation type WITH standard errors (for bar chart)\nepistemic_data = []\nfor pert_type, config in PERTURBATION_CONFIG.items():\n    pert_data = merged[merged['perturbation_type'] == pert_type]\n    if len(pert_data) == 0:\n        continue\n    \n    mean_delta = pert_data['net_delta'].mean()\n    std_delta = pert_data['net_delta'].std()\n    n = len(pert_data)\n    se = std_delta / np.sqrt(n)\n    ci = z * se\n    \n    epistemic_data.append({\n        'perturbation': config['name'],\n        'category': config['category'],\n        'net_delta': mean_delta,\n        'std': std_delta,\n        'se': se,\n        'ci': ci,\n        'flip_pct': pert_data['verdict_flipped'].mean() * 100 if 'verdict_flipped' in pert_data.columns else 0,\n        'n': n\n    })\n\nepistemic_df = pd.DataFrame(epistemic_data)\n\n# Sort by category then net_delta\ncategory_order = ['Persuasion', 'Surface', 'Point-of-view']\nepistemic_df['category'] = pd.Categorical(\n    epistemic_df['category'], \n    categories=category_order, \n    ordered=True\n)\nepistemic_df = epistemic_df.sort_values(['category', 'net_delta'], ascending=[True, True])\n\nprint(\"Epistemic Stance by Perturbation (with 95% CI):\\n\")\nprint(epistemic_df[['perturbation', 'category', 'net_delta', 'ci', 'n']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part B: Verification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification annotations: 10,702\n",
      "Reasoning protocols: 13,317\n"
     ]
    }
   ],
   "source": [
    "# Load verification data\n",
    "verif_df = pd.read_parquet('../data/verification_annotations.parquet')\n",
    "verif_df = verif_df[verif_df['error'].isna()].copy()\n",
    "verif_df = verif_df.drop_duplicates(subset=['scenario_id', 'model', 'protocol'])\n",
    "verif_df['verif_bin'] = verif_df['verification'].str.lower() == 'yes'\n",
    "\n",
    "# Load reasoning protocols\n",
    "reasoning_df = pd.read_parquet('../data/reasoning_protocols_combined_validated.parquet')\n",
    "\n",
    "print(f\"Verification annotations: {len(verif_df):,}\")\n",
    "print(f\"Reasoning protocols: {len(reasoning_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model columns: ['claude-thinking', 'deepseek-r1', 'o3-mini', 'qwq-32b']\n",
      "\n",
      "Cross-Model Agreement by Verification Level:\n",
      "       level  agreement       ci   n\n",
      "    None (0)   0.983740 0.038713  41\n",
      "   Low (1-3)   0.669020 0.044738 425\n",
      "Medium (4-6)   0.263957 0.034836 615\n",
      "   High (7+)   0.154062 0.064862 119\n",
      "\n",
      "Correlation: r = -0.559, p = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Get scenario-level verification\n",
    "scenario_total_verif = verif_df.groupby('scenario_id')['verif_bin'].sum().reset_index()\n",
    "scenario_total_verif.columns = ['scenario_id', 'total_verifications']\n",
    "\n",
    "# Calculate cross-model agreement per scenario\n",
    "cross_model = reasoning_df.pivot_table(\n",
    "    index=['scenario_id', 'protocol'],\n",
    "    columns='model',\n",
    "    values='judgment',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "\n",
    "# Check which columns exist\n",
    "model_cols = [c for c in cross_model.columns if c not in ['scenario_id', 'protocol']]\n",
    "print(f\"Model columns: {model_cols}\")\n",
    "\n",
    "# Calculate agreement (all models agree)\n",
    "if len(model_cols) >= 2:\n",
    "    cross_model['models_agree'] = cross_model[model_cols].nunique(axis=1) == 1\n",
    "    cross_model['models_agree'] = cross_model['models_agree'].astype(int)\n",
    "else:\n",
    "    cross_model['models_agree'] = 1\n",
    "\n",
    "# Aggregate to scenario level\n",
    "scenario_agreement = cross_model.groupby('scenario_id')['models_agree'].mean().reset_index()\n",
    "scenario_agreement.columns = ['scenario_id', 'cross_model_agreement']\n",
    "\n",
    "# Merge\n",
    "scenario_analysis = scenario_agreement.merge(scenario_total_verif, on='scenario_id')\n",
    "\n",
    "# Bin by total verifications\n",
    "scenario_analysis['verif_level'] = pd.cut(\n",
    "    scenario_analysis['total_verifications'],\n",
    "    bins=[-1, 0, 3, 6, 100],\n",
    "    labels=['None (0)', 'Low (1-3)', 'Medium (4-6)', 'High (7+)']\n",
    ")\n",
    "\n",
    "# Calculate stats by level\n",
    "level_stats = []\n",
    "for level in ['None (0)', 'Low (1-3)', 'Medium (4-6)', 'High (7+)']:\n",
    "    subset = scenario_analysis[scenario_analysis['verif_level'] == level]\n",
    "    if len(subset) > 5:\n",
    "        agree = subset['cross_model_agreement'].mean()\n",
    "        level_stats.append({\n",
    "            'level': level,\n",
    "            'agreement': agree,\n",
    "            'ci': calculate_ci(agree, len(subset)),\n",
    "            'n': len(subset)\n",
    "        })\n",
    "\n",
    "level_df = pd.DataFrame(level_stats)\n",
    "\n",
    "# Correlation\n",
    "r, p = pointbiserialr(scenario_analysis['total_verifications'], scenario_analysis['cross_model_agreement'])\n",
    "\n",
    "print(\"\\nCross-Model Agreement by Verification Level:\")\n",
    "print(level_df.to_string(index=False))\n",
    "print(f\"\\nCorrelation: r = {r:.3f}, p = {p:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Combined Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Create combined figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5.5), dpi=150)\n\n# =============================================================================\n# SHARED STYLING\n# =============================================================================\nCATEGORY_COLORS = {\n    'Point-of-view': '#222222',  # Black\n    'Persuasion': '#888888',      # Gray\n    'Surface': '#dddddd'          # Light Gray\n}\n\nBAR_EDGECOLOR = '#222222'\nBAR_LINEWIDTH = 0.5\nERROR_CAPSIZE = 3\nERROR_LINEWIDTH = 1.2\n\nLEGEND_ORDER = ['Point-of-view', 'Persuasion', 'Surface']\n\n# =============================================================================\n# PANEL A: Epistemic Stance Change (Horizontal Bar Chart with 95% CIs)\n# =============================================================================\ny_positions = np.arange(len(epistemic_df))\ncolors_a = [CATEGORY_COLORS[cat] for cat in epistemic_df['category']]\n\n# Horizontal bar chart with error bars\nbars = ax1.barh(y_positions, epistemic_df['net_delta'], xerr=epistemic_df['ci'],\n                color=colors_a, edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH, \n                height=0.7, capsize=ERROR_CAPSIZE, \n                error_kw={'lw': ERROR_LINEWIDTH, 'capthick': ERROR_LINEWIDTH})\n\n# Zero line\nax1.axvline(x=0, color='#222222', linewidth=0.8)\n\n# Labels\nax1.set_yticks(y_positions)\nax1.set_yticklabels(epistemic_df['perturbation'])\nax1.set_xlabel('$\\\\Delta$ Net Epistemic Stance (per 100 words)', fontweight='bold')\nax1.set_title('(A) Epistemic Stance Change by Perturbation', fontweight='bold', loc='left')\n\n# Category separators\nprev_cat = None\nfor i, (_, row) in enumerate(epistemic_df.iterrows()):\n    if row['category'] != prev_cat and prev_cat is not None:\n        ax1.axhline(y=i-0.5, color='#cccccc', linewidth=0.5, linestyle='--')\n    prev_cat = row['category']\n\n# Legend with shared colors\nlegend_patches = [mpatches.Patch(color=CATEGORY_COLORS[cat], label=cat,\n                                  edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH)\n                  for cat in LEGEND_ORDER]\nax1.legend(handles=legend_patches, loc='upper right', framealpha=0.9)\n\n# Styling - expand x limits to accommodate error bars\nxmin = min(epistemic_df['net_delta'].min() - epistemic_df['ci'].max(), -0.06) * 1.3\nxmax = max(epistemic_df['net_delta'].max() + epistemic_df['ci'].max(), 0.12) * 1.3\nax1.set_xlim(xmin, xmax)\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.invert_yaxis()\nax1.grid(axis='x', alpha=0.3, linestyle='--')\n\n# Directional labels\nax1.text(xmin * 0.85, -0.65, '$\\\\leftarrow$ More hedged', fontsize=8, ha='left', style='italic', color='#666666')\nax1.text(xmax * 0.85, -0.65, 'More confident $\\\\rightarrow$', fontsize=8, ha='right', style='italic', color='#666666')\n\n# =============================================================================\n# PANEL B: Cross-Model Agreement by Verification Level\n# =============================================================================\n\nx = np.arange(len(level_df))\nwidth = 0.6\ncolors_b = ['#333333', '#555555', '#888888', '#bbbbbb']\n\nbars = ax2.bar(x, level_df['agreement'] * 100, width,\n               color=colors_b, edgecolor='#222222', linewidth=0.8)\nax2.errorbar(x, level_df['agreement'] * 100,\n             yerr=level_df['ci'] * 100,\n             fmt='none', ecolor='#222222', capsize=4, lw=1.5)\n\n# Trend line\nz_fit = np.polyfit(x, level_df['agreement'] * 100, 1)\np_fit = np.poly1d(z_fit)\nax2.plot(x, p_fit(x), 'k--', lw=2, alpha=0.7, label=f'r = {r:.2f}')\n\nax2.set_ylabel('Cross-Model Agreement (%)', fontweight='bold')\nax2.set_xlabel('Scenario Verification Level', fontweight='bold')\nax2.set_title('(B) Cross-Model Agreement by Scenario Verification', fontweight='bold', loc='left')\nax2.set_xticks(x)\nax2.set_xticklabels(level_df['level'], fontsize=10)\nax2.legend(loc='upper right', framealpha=0.9)\nax2.set_ylim(0, 100)\nax2.grid(axis='y', alpha=0.3, linestyle='--')\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../figures/fig_epistemic_verification_combined.pdf\n",
      "Saved: ../../figures/fig_epistemic_verification_combined.png\n"
     ]
    }
   ],
   "source": [
    "# Save figure\n",
    "output_dir = Path('../../figures')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_path = output_dir / 'fig_epistemic_verification_combined.pdf'\n",
    "png_path = output_dir / 'fig_epistemic_verification_combined.png'\n",
    "\n",
    "fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(f\"Saved: {pdf_path}\")\n",
    "print(f\"Saved: {png_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "print(\"KEY INSIGHTS\")\nprint(\"=\" * 70)\nprint(\"\\nPanel A (Epistemic Stance Change by Perturbation):\")\nprint(\"  - Horizontal bars show delta net epistemic stance (boosters - hedges)\")\nprint(\"  - Push Self At Fault: More hedged (negative delta) - uncertainty\")\nprint(\"  - Surface: Minimal effect - semantic irrelevance\")\nprint(\"  - Point-of-view: Most confident (positive delta) - third-person = direct\")\nprint(\"  - All effects shown with 95% CIs\")\nprint(\"\\nPanel B (Cross-Model Agreement):\")\nprint(f\"  - High-verification scenarios: {level_df[level_df['level']=='High (7+)']['agreement'].values[0]*100:.0f}% agreement\")\nprint(f\"  - No-verification scenarios: {level_df[level_df['level']=='None (0)']['agreement'].values[0]*100:.0f}% agreement\")\nprint(f\"  - Correlation: r = {r:.2f}\")\nprint(\"  - Verification marks scenario-level ambiguity\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}