{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Combined Figure: Verdict Instability + Epistemic Stance\n",
    "\n",
    "Two-panel figure combining:\n",
    "- **Panel A**: Flip rates by base verdict category and perturbation family\n",
    "- **Panel B**: Net epistemic stance change by perturbation type (with 95% CIs)\n",
    "\n",
    "**Purpose**: Shows both verdict-level instability patterns and how explanatory language shifts under perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats import norm\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication settings\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 11,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 9,\n",
    "    'pdf.fonttype': 42,\n",
    "    'ps.fonttype': 42,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Define Epistemic Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedges: 14, Boosters: 10\n"
     ]
    }
   ],
   "source": [
    "# Epistemic stance lexicon inspired by LIWC2015 tentative/certainty categories,\n",
    "# restricted to high-precision epistemic markers (propositional uncertainty,\n",
    "# not politeness/suggestion hedging like \"you could try...\")\n",
    "EPISTEMIC_HEDGES = [\n",
    "    'seem*', 'appear*',           # Evidential verbs (note: \"appear\" has perceptual polysemy)\n",
    "    'might', 'could', 'may',      # Epistemic modals (note: \"may\" has month/permission ambiguity)\n",
    "    'perhaps', 'possibly', 'maybe',  # Epistemic adverbs\n",
    "    'probably', 'likely',         # Probability markers\n",
    "    'unclear', 'uncertain', 'unsure',  # Explicit uncertainty\n",
    "    'guess*',                     # LIWC tentat exemplar\n",
    "]\n",
    "\n",
    "EPISTEMIC_BOOSTERS = [\n",
    "    'clearly', 'obviously',       # Evidential emphasis\n",
    "    'definitely', 'certainly',    # Certainty adverbs\n",
    "    'undoubtedly', 'unquestionably',  # Strong certainty\n",
    "    'absolutely',                 # Emphasis\n",
    "    'always', 'never',            # Categorical terms (LIWC certitude anchors)\n",
    "    'sure',                       # Confidence marker\n",
    "]\n",
    "\n",
    "NEGATION_WORDS = ['not', 'no', \"n't\", 'never', 'neither', 'nor']\n",
    "\n",
    "def term_to_pattern(term: str) -> str:\n",
    "    \"\"\"Convert term to regex pattern.\"\"\"\n",
    "    term = term.lower()\n",
    "    if term.endswith('*'):\n",
    "        return r'\\b' + re.escape(term[:-1]) + r'\\w*\\b'\n",
    "    return r'\\b' + re.escape(term) + r'\\b'\n",
    "\n",
    "def is_negated(text: str, match_start: int, window: int = 3) -> bool:\n",
    "    \"\"\"Check if match is negated within word window.\"\"\"\n",
    "    before = text[:match_start].lower().split()[-window:]\n",
    "    return any(w in NEGATION_WORDS or \"n't\" in w for w in before)\n",
    "\n",
    "def count_markers(text: str, markers: list) -> int:\n",
    "    \"\"\"Count marker occurrences, excluding negated instances.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = text.lower()\n",
    "    count = 0\n",
    "    for term in markers:\n",
    "        pattern = term_to_pattern(term)\n",
    "        try:\n",
    "            for match in re.finditer(pattern, text_lower):\n",
    "                if not is_negated(text_lower, match.start()):\n",
    "                    count += 1\n",
    "        except re.error:\n",
    "            continue\n",
    "    return count\n",
    "\n",
    "def compute_net_epistemic(text: str) -> float:\n",
    "    \"\"\"Compute net epistemic stance per 100 words.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return 0.0\n",
    "    words = len(text.split())\n",
    "    if words == 0:\n",
    "        return 0.0\n",
    "    hedges = count_markers(text, EPISTEMIC_HEDGES)\n",
    "    boosters = count_markers(text, EPISTEMIC_BOOSTERS)\n",
    "    return ((boosters - hedges) / words) * 100\n",
    "\n",
    "print(f\"Hedges: {len(EPISTEMIC_HEDGES)}, Boosters: {len(EPISTEMIC_BOOSTERS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flip rates: 15 rows\n",
      "Loading master parquet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master data: 164,424 rows\n"
     ]
    }
   ],
   "source": [
    "# Load flip rates data for Panel A\n",
    "results_dir = Path('../../results/analysis')\n",
    "flip_rates = pd.read_csv(results_dir / 'flip_rates_by_base_verdict.csv')\n",
    "print(f\"Flip rates: {len(flip_rates)} rows\")\n",
    "\n",
    "# Load master data for Panel B (to compute CIs)\n",
    "print(\"Loading master parquet...\")\n",
    "df = pd.read_parquet('../../final_results/parquet/master_final_model_own_baseline.parquet')\n",
    "print(f\"Master data: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Prepare Panel A Data (Flip Rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for Panel A\npivot_flip = flip_rates.pivot(\n    index='base_verdict',\n    columns='perturbation_family',\n    values='flip_rate'\n)\n\n# Exclude \"Unclear\" verdict\nif 'Unclear' in pivot_flip.index:\n    pivot_flip = pivot_flip.drop('Unclear')\n\n# Order by average flip rate (descending)\npivot_flip['Average'] = pivot_flip.mean(axis=1)\npivot_flip = pivot_flip.sort_values('Average', ascending=False)\npivot_flip = pivot_flip[['Presentation', 'Psychological', 'Robustness']]\n\n# Rename columns to match paper terminology\npivot_flip = pivot_flip.rename(columns={\n    'Presentation': 'Point-of-view',\n    'Psychological': 'Persuasion',\n    'Robustness': 'Surface'\n})\n\n# Rename verdict labels to full names (using paper conventions with underscores replaced)\nverdict_labels = {\n    'No_One_At_Fault': 'No One At Fault',\n    'All_At_Fault': 'All At Fault',\n    'Self_At_Fault': 'Self At Fault',\n    'Other_At_Fault': 'Other At Fault'\n}\npivot_flip.index = [verdict_labels.get(v, v) for v in pivot_flip.index]\n\nprint(\"Flip Rates by Base Verdict (%):\\n\")\nprint(pivot_flip.round(2))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Prepare Panel B Data (Epistemic Stance with CIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing epistemic stance (this may take 1-2 minutes)...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Compute epistemic stance for all explanations\n",
    "print(\"Computing epistemic stance (this may take 1-2 minutes)...\")\n",
    "df['net_epistemic'] = df['explanation'].apply(compute_net_epistemic)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Get baseline data\nbaseline = df[df['perturbation_type'] == 'none'].copy()\nbaseline_rates = baseline.groupby(['id', 'model', 'run_number']).agg({\n    'net_epistemic': 'first'\n}).reset_index()\nbaseline_rates.columns = ['id', 'model', 'run_number', 'base_epistemic']\n\n# Perturbation config - using paper terminology for categories\nPERTURBATION_CONFIG = {\n    'push_yta_social_proof': {'name': 'Social proof (against)', 'category': 'Persuasion'},\n    'push_yta_pattern_admission': {'name': 'Pattern admission', 'category': 'Persuasion'},\n    'push_yta_self_condemning': {'name': 'Self-condemning', 'category': 'Persuasion'},\n    'change_trivial_detail': {'name': 'Change trivial detail', 'category': 'Surface'},\n    'add_extraneous_detail': {'name': 'Add extraneous detail', 'category': 'Surface'},\n    'remove_sentence': {'name': 'Remove sentence', 'category': 'Surface'},\n    'push_nta_victim_pattern': {'name': 'Victim pattern', 'category': 'Persuasion'},\n    'push_nta_self_justifying': {'name': 'Self-justifying', 'category': 'Persuasion'},\n    'push_nta_social_proof': {'name': 'Social proof (for)', 'category': 'Persuasion'},\n    'firstperson_atfault': {'name': 'First-person', 'category': 'Point-of-view'},\n    'thirdperson': {'name': 'Third-person', 'category': 'Point-of-view'},\n}\n\nperturbations = df[df['perturbation_type'].isin(PERTURBATION_CONFIG.keys())].copy()\nprint(f\"Perturbation records: {len(perturbations):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched pairs: 129,156\n"
     ]
    }
   ],
   "source": [
    "# Merge and compute deltas\n",
    "merged = perturbations.merge(baseline_rates, on=['id', 'model', 'run_number'], how='inner')\n",
    "merged['net_delta'] = merged['net_epistemic'] - merged['base_epistemic']\n",
    "print(f\"Matched pairs: {len(merged):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate by perturbation type WITH standard errors\nz = norm.ppf(0.975)  # 1.96 for 95% CI\n\nresults = []\nfor pert_type, config in PERTURBATION_CONFIG.items():\n    pert_data = merged[merged['perturbation_type'] == pert_type]\n    if len(pert_data) == 0:\n        continue\n    \n    mean_delta = pert_data['net_delta'].mean()\n    std_delta = pert_data['net_delta'].std()\n    n = len(pert_data)\n    se = std_delta / np.sqrt(n)\n    ci = z * se\n    \n    results.append({\n        'perturbation': config['name'],\n        'category': config['category'],\n        'net_delta': mean_delta,\n        'std': std_delta,\n        'se': se,\n        'ci': ci,\n        'flip_pct': pert_data['verdict_flipped'].mean() * 100,\n        'n': n\n    })\n\nepistemic_data = pd.DataFrame(results)\n\n# Sort by category then net_delta - using paper terminology\ncategory_order = ['Persuasion', 'Surface', 'Point-of-view']\nepistemic_data['category'] = pd.Categorical(\n    epistemic_data['category'], \n    categories=category_order, \n    ordered=True\n)\nepistemic_data = epistemic_data.sort_values(['category', 'net_delta'], ascending=[True, True])\n\nprint(\"Epistemic Stance by Perturbation (with 95% CI):\\n\")\nprint(epistemic_data[['perturbation', 'category', 'net_delta', 'ci', 'flip_pct', 'n']].to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Create Combined Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Create combined two-panel figure\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5), dpi=150)\n\n# =============================================================================\n# SHARED STYLING - consistent across both panels\n# =============================================================================\nCATEGORY_COLORS = {\n    'Point-of-view': '#222222',  # Black\n    'Persuasion': '#888888',      # Gray\n    'Surface': '#dddddd'          # Light Gray\n}\n\n# Shared bar/error styling\nBAR_EDGECOLOR = '#222222'\nBAR_LINEWIDTH = 0.5\nERROR_CAPSIZE = 3\nERROR_LINEWIDTH = 1.2\n\n# Legend order (consistent across panels)\nLEGEND_ORDER = ['Point-of-view', 'Persuasion', 'Surface']\n\n# =============================================================================\n# PANEL A: Flip Rates by Base Verdict\n# =============================================================================\nx = np.arange(len(pivot_flip))\nwidth = 0.25\n\n# Reverse mapping for error bar lookup (full label -> original CSV label)\nlabel_to_csv = {\n    'No One At Fault': 'No_One_At_Fault',\n    'All At Fault': 'All_At_Fault',\n    'Self At Fault': 'Self_At_Fault',\n    'Other At Fault': 'Other_At_Fault'\n}\n\ncol_to_csv = {\n    'Point-of-view': 'Presentation',\n    'Persuasion': 'Psychological',\n    'Surface': 'Robustness'\n}\n\n# Plot bars in legend order\nfor i, col in enumerate(LEGEND_ORDER):\n    offset = (i - 1) * width\n    bars = ax1.bar(x + offset, pivot_flip[col], width, label=col, \n                   color=CATEGORY_COLORS[col], \n                   edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH)\n\n# Add confidence intervals for Panel A\nfor i, col in enumerate(LEGEND_ORDER):\n    for j, verdict in enumerate(pivot_flip.index):\n        csv_col = col_to_csv.get(col, col)\n        csv_verdict = label_to_csv.get(verdict, verdict)\n        mask = (flip_rates['base_verdict'] == csv_verdict) & \\\n               (flip_rates['perturbation_family'] == csv_col)\n        row = flip_rates[mask]\n        if not row.empty:\n            n = row['n_total'].values[0]\n            p = row['flip_rate'].values[0] / 100\n            se = np.sqrt(p * (1 - p) / n) * 100\n            ci = z * se\n            offset = (i - 1) * width\n            ax1.errorbar(j + offset, pivot_flip.loc[verdict, col], yerr=ci, \n                         fmt='none', ecolor='black', capsize=ERROR_CAPSIZE, \n                         lw=ERROR_LINEWIDTH, capthick=ERROR_LINEWIDTH, zorder=10)\n\nax1.set_xlabel('Base Verdict Category', fontweight='bold')\nax1.set_ylabel('Flip Rate (%)', fontweight='bold')\nax1.set_title('(A) Verdict Instability by Base Judgment', fontweight='bold', loc='left')\nax1.set_xticks(x)\nax1.set_xticklabels(pivot_flip.index, fontsize=9)\nax1.legend(loc='upper right', framealpha=0.9, bbox_to_anchor=(1, 0.97))\nax1.set_ylim(0, 65)\nax1.grid(axis='y', alpha=0.3, linestyle='--')\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\n\n# =============================================================================\n# PANEL B: Epistemic Stance Change (with 95% CIs)\n# =============================================================================\ny_positions = np.arange(len(epistemic_data))\ncolors_b = [CATEGORY_COLORS[cat] for cat in epistemic_data['category']]\n\n# Horizontal bar chart with error bars\nbars = ax2.barh(y_positions, epistemic_data['net_delta'], xerr=epistemic_data['ci'],\n                color=colors_b, edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH, \n                height=0.7, capsize=ERROR_CAPSIZE, \n                error_kw={'lw': ERROR_LINEWIDTH, 'capthick': ERROR_LINEWIDTH})\n\n# Zero line\nax2.axvline(x=0, color='#222222', linewidth=0.8)\n\n# Labels\nax2.set_yticks(y_positions)\nax2.set_yticklabels(epistemic_data['perturbation'])\nax2.set_xlabel('$\\\\Delta$ Net Epistemic Stance (per 100 words)', fontweight='bold')\nax2.set_title('(B) Epistemic Stance Change by Perturbation', fontweight='bold', loc='left')\n\n# Category separators\nprev_cat = None\nfor i, (_, row) in enumerate(epistemic_data.iterrows()):\n    if row['category'] != prev_cat and prev_cat is not None:\n        ax2.axhline(y=i-0.5, color='#cccccc', linewidth=0.5, linestyle='--')\n    prev_cat = row['category']\n\n# Flip rate annotations (positioned after error bars)\nfor i, (_, row) in enumerate(epistemic_data.iterrows()):\n    x_pos = row['net_delta']\n    ci = row['ci']\n    # Position text after the error bar\n    if x_pos >= 0:\n        text_x = x_pos + ci + 0.005\n        ha = 'left'\n    else:\n        text_x = x_pos - ci - 0.005\n        ha = 'right'\n    ax2.annotate(f\"{row['flip_pct']:.0f}%\",\n                 xy=(text_x, i),\n                 fontsize=8, va='center', ha=ha, color='#555555')\n\n# Legend with shared colors (same order as Panel A)\nlegend_patches = [mpatches.Patch(color=CATEGORY_COLORS[cat], label=cat,\n                                  edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH)\n                  for cat in LEGEND_ORDER]\nax2.legend(handles=legend_patches, loc='upper right', framealpha=0.9, bbox_to_anchor=(1, 0.97))\n\n# Styling - expand x limits to accommodate error bars and annotations\nxmin = min(epistemic_data['net_delta'].min() - epistemic_data['ci'].max(), -0.06) * 1.5\nxmax = max(epistemic_data['net_delta'].max() + epistemic_data['ci'].max(), 0.12) * 1.5\nax2.set_xlim(xmin, xmax)\nax2.spines['top'].set_visible(False)\nax2.spines['right'].set_visible(False)\nax2.invert_yaxis()\nax2.grid(axis='x', alpha=0.3, linestyle='--')\n\n# Directional labels\nax2.text(xmin * 0.85, -0.65, '$\\\\leftarrow$ More hedged', fontsize=8, ha='left', style='italic', color='#666666')\nax2.text(xmax * 0.85, -0.65, 'More confident $\\\\rightarrow$', fontsize=8, ha='right', style='italic', color='#666666')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Save Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../figures/fig_instability_epistemic_combined.pdf\n",
      "Saved: ../../figures/fig_instability_epistemic_combined.png\n"
     ]
    }
   ],
   "source": [
    "# Save combined figure\n",
    "output_dir = Path('../../figures')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_path = output_dir / 'fig_instability_epistemic_combined.pdf'\n",
    "png_path = output_dir / 'fig_instability_epistemic_combined.png'\n",
    "\n",
    "fig.savefig(pdf_path, bbox_inches='tight', dpi=300)\n",
    "fig.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "print(f\"Saved: {pdf_path}\")\n",
    "print(f\"Saved: {png_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "Panel A (Verdict Instability by Base Judgment):\n",
      "  - NAF (No One At Fault) is most unstable across all perturbation types\n",
      "  - OAF (Other At Fault) is most stable\n",
      "  - Point-of-view perturbations cause highest flip rates\n",
      "  - Models are LEAST stable when moral reasoning should be MOST careful\n",
      "\n",
      "Panel B (Epistemic Stance Change):\n",
      "  - Push-SAF: More hedged (-0.014 to -0.046) - uncertainty when narrator self-criticizes\n",
      "  - Surface: Minimal effect (-0.025 to +0.000) - semantic irrelevance\n",
      "  - Push-OAF: Slightly more confident (+0.013 to +0.031)\n",
      "  - Point-of-view: MOST confident (+0.048 to +0.082) - third-person = direct language\n",
      "  - All effects are statistically significant (CIs don't cross zero for most)\n",
      "\n",
      "Lexicon (LIWC2015-inspired, high-precision):\n",
      "  - Hedges: seem*, appear*, might, could, may, perhaps, possibly, maybe,\n",
      "            probably, likely, unclear, uncertain, unsure, guess*\n",
      "  - Boosters: clearly, obviously, definitely, certainly, undoubtedly,\n",
      "              unquestionably, absolutely, always, never, sure\n",
      "\n",
      "CONNECTION:\n",
      "  - Point-of-view perturbations cause both highest flip rates AND most confident language\n",
      "  - Suggests frame shifts induce different 'reasoning modes' in models\n",
      "  - Third-person narration licenses more assertive moral assessments\n"
     ]
    }
   ],
   "source": [
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nPanel A (Verdict Instability by Base Judgment):\")\n",
    "print(\"  - NAF (No One At Fault) is most unstable across all perturbation types\")\n",
    "print(\"  - OAF (Other At Fault) is most stable\")\n",
    "print(\"  - Point-of-view perturbations cause highest flip rates\")\n",
    "print(\"  - Models are LEAST stable when moral reasoning should be MOST careful\")\n",
    "\n",
    "print(\"\\nPanel B (Epistemic Stance Change):\")\n",
    "print(\"  - Push-SAF: More hedged (-0.014 to -0.046) - uncertainty when narrator self-criticizes\")\n",
    "print(\"  - Surface: Minimal effect (-0.025 to +0.000) - semantic irrelevance\")\n",
    "print(\"  - Push-OAF: Slightly more confident (+0.013 to +0.031)\")\n",
    "print(\"  - Point-of-view: MOST confident (+0.048 to +0.082) - third-person = direct language\")\n",
    "print(\"  - All effects are statistically significant (CIs don't cross zero for most)\")\n",
    "\n",
    "print(\"\\nLexicon (LIWC2015-inspired, high-precision):\")\n",
    "print(\"  - Hedges: seem*, appear*, might, could, may, perhaps, possibly, maybe,\")\n",
    "print(\"            probably, likely, unclear, uncertain, unsure, guess*\")\n",
    "print(\"  - Boosters: clearly, obviously, definitely, certainly, undoubtedly,\")\n",
    "print(\"              unquestionably, absolutely, always, never, sure\")\n",
    "\n",
    "print(\"\\nCONNECTION:\")\n",
    "print(\"  - Point-of-view perturbations cause both highest flip rates AND most confident language\")\n",
    "print(\"  - Suggests frame shifts induce different 'reasoning modes' in models\")\n",
    "print(\"  - Third-person narration licenses more assertive moral assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jrsc8l7qaqi",
   "source": "## Alternative Panel B: Narrator Blame Direction\n\nInstead of epistemic stance, this panel shows the **asymmetric effectiveness** of perturbations:\n- **Blame direction ratio**: (transitions toward narrator blame) / (transitions toward exoneration)\n- Ratio > 1 = net movement toward blaming the narrator\n- Ratio < 1 = net movement toward exonerating the narrator\n- Ratio ≈ 1 = balanced (perturbation has no directional effect)\n\n**Key finding**: Push Self At Fault perturbations work as intended (~4-6x toward blame), but Push Other At Fault perturbations fail or backfire (~1x toward blame instead of exoneration).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "irdx0kvhhm",
   "source": "# =============================================================================\n# Compute Blame Direction Data\n# =============================================================================\n\n# Define OP blame status groups\nOP_BLAMED = {'Self_At_Fault', 'All_At_Fault'}\nOP_EXONERATED = {'Other_At_Fault', 'No_One_At_Fault'}\n\ndef get_op_transition(base_v, target_v):\n    \"\"\"Classify transition by OP blame status change.\"\"\"\n    if pd.isna(base_v) or pd.isna(target_v):\n        return None\n    base_blamed = base_v in OP_BLAMED\n    target_blamed = target_v in OP_BLAMED\n    \n    if base_blamed == target_blamed:\n        return 'preserved'  # stays in same blame-status group\n    elif not base_blamed and target_blamed:\n        return 'newly_blamed'  # exonerated -> blamed\n    else:\n        return 'exonerated'  # blamed -> exonerated\n\n# Get baseline verdicts\nbaseline_verdicts = df[df['perturbation_type'] == 'none'][['id', 'model', 'run_number', 'standardized_judgment']].copy()\nbaseline_verdicts = baseline_verdicts.rename(columns={'standardized_judgment': 'baseline_verdict'})\n\n# Merge with perturbations\nperturbed = df[df['perturbation_type'].isin(PERTURBATION_CONFIG.keys())].copy()\nmerged_blame = perturbed.merge(baseline_verdicts, on=['id', 'model', 'run_number'], how='inner')\n\n# Only look at flips\nflips = merged_blame[merged_blame['verdict_flipped'] == True].copy()\nflips['op_transition'] = flips.apply(\n    lambda r: get_op_transition(r['baseline_verdict'], r['standardized_judgment']), axis=1\n)\n\nprint(f\"Total flips: {len(flips):,}\")\nprint(f\"Transition breakdown:\")\nprint(flips['op_transition'].value_counts())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3pgpitcrhsz",
   "source": "# =============================================================================\n# Aggregate blame direction by perturbation type\n# =============================================================================\n\n# Extended config with direction info - using paper terminology\nPERT_DIRECTION_CONFIG = {\n    # Push Self At Fault (designed to increase narrator blame)\n    'push_yta_social_proof': {'name': 'Social proof (against)', 'category': 'Persuasion: Push Self At Fault', 'intended': 'blame'},\n    'push_yta_pattern_admission': {'name': 'Pattern admission', 'category': 'Persuasion: Push Self At Fault', 'intended': 'blame'},\n    'push_yta_self_condemning': {'name': 'Self-condemning', 'category': 'Persuasion: Push Self At Fault', 'intended': 'blame'},\n    # Push Other At Fault (designed to decrease narrator blame / exonerate)\n    'push_nta_victim_pattern': {'name': 'Victim pattern', 'category': 'Persuasion: Push Other At Fault', 'intended': 'exonerate'},\n    'push_nta_self_justifying': {'name': 'Self-justifying', 'category': 'Persuasion: Push Other At Fault', 'intended': 'exonerate'},\n    'push_nta_social_proof': {'name': 'Social proof (for)', 'category': 'Persuasion: Push Other At Fault', 'intended': 'exonerate'},\n    # Presentation (neutral - testing frame effects)\n    'firstperson_atfault': {'name': 'First-person', 'category': 'Point-of-view', 'intended': 'neutral'},\n    'thirdperson': {'name': 'Third-person', 'category': 'Point-of-view', 'intended': 'neutral'},\n    # Surface (neutral - robustness test)\n    'change_trivial_detail': {'name': 'Change trivial detail', 'category': 'Surface', 'intended': 'neutral'},\n    'add_extraneous_detail': {'name': 'Add extraneous detail', 'category': 'Surface', 'intended': 'neutral'},\n    'remove_sentence': {'name': 'Remove sentence', 'category': 'Surface', 'intended': 'neutral'},\n}\n\n# Compute blame direction stats by perturbation\nblame_results = []\nfor pert_type, config in PERT_DIRECTION_CONFIG.items():\n    pert_flips = flips[flips['perturbation_type'] == pert_type]\n    if len(pert_flips) == 0:\n        continue\n    \n    n_total = len(pert_flips)\n    n_newly_blamed = (pert_flips['op_transition'] == 'newly_blamed').sum()\n    n_exonerated = (pert_flips['op_transition'] == 'exonerated').sum()\n    n_preserved = (pert_flips['op_transition'] == 'preserved').sum()\n    \n    # Compute ratio (add small constant to avoid division by zero)\n    ratio = n_newly_blamed / max(n_exonerated, 1)\n    \n    # Compute 95% CI for ratio using log transformation\n    # SE(log(ratio)) ≈ sqrt(1/a + 1/b) for ratio = a/b\n    if n_newly_blamed > 0 and n_exonerated > 0:\n        se_log = np.sqrt(1/n_newly_blamed + 1/n_exonerated)\n        ci_lower = np.exp(np.log(ratio) - 1.96 * se_log)\n        ci_upper = np.exp(np.log(ratio) + 1.96 * se_log)\n    else:\n        ci_lower = ci_upper = ratio\n    \n    blame_results.append({\n        'perturbation': config['name'],\n        'category': config['category'],\n        'intended': config['intended'],\n        'n_flips': n_total,\n        'n_newly_blamed': n_newly_blamed,\n        'n_exonerated': n_exonerated,\n        'n_preserved': n_preserved,\n        'ratio': ratio,\n        'ci_lower': ci_lower,\n        'ci_upper': ci_upper,\n        'net': n_newly_blamed - n_exonerated,\n    })\n\nblame_data = pd.DataFrame(blame_results)\n\n# Sort: Push Self At Fault first (descending by ratio), then Push Other At Fault, then others\ncategory_order = ['Persuasion: Push Self At Fault', 'Persuasion: Push Other At Fault', 'Point-of-view', 'Surface']\nblame_data['category'] = pd.Categorical(blame_data['category'], categories=category_order, ordered=True)\nblame_data = blame_data.sort_values(['category', 'ratio'], ascending=[True, False])\n\nprint(\"Blame Direction by Perturbation Type:\")\nprint(blame_data[['perturbation', 'category', 'n_newly_blamed', 'n_exonerated', 'ratio', 'net']].to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uafg31khzgj",
   "source": "# =============================================================================\n# Create Combined Figure: Panel A (Flip Rates) + Panel B (Blame Direction)\n# =============================================================================\n\nfig2, (ax1_alt, ax2_alt) = plt.subplots(1, 2, figsize=(14, 5), dpi=150)\n\n# =============================================================================\n# SHARED STYLING\n# =============================================================================\n# Grayscale colors with hatching patterns for distinction\nCATEGORY_STYLES = {\n    'Persuasion: Push Self At Fault': {'color': '#444444', 'hatch': ''},\n    'Persuasion: Push Other At Fault': {'color': '#777777', 'hatch': ''},\n    'Point-of-view': {'color': '#aaaaaa', 'hatch': ''},\n    'Surface': {'color': '#dddddd', 'hatch': ''}\n}\n\nBAR_EDGECOLOR = '#222222'\nBAR_LINEWIDTH = 0.5\nERROR_CAPSIZE = 3\nERROR_LINEWIDTH = 1.2\n\n# =============================================================================\n# PANEL A: Flip Rates by Base Verdict (same as before)\n# =============================================================================\nx = np.arange(len(pivot_flip))\nwidth = 0.25\n\nCATEGORY_COLORS_A = {\n    'Point-of-view': '#222222',\n    'Persuasion': '#888888',\n    'Surface': '#dddddd'\n}\nLEGEND_ORDER_A = ['Point-of-view', 'Persuasion', 'Surface']\n\nfor i, col in enumerate(LEGEND_ORDER_A):\n    offset = (i - 1) * width\n    bars = ax1_alt.bar(x + offset, pivot_flip[col], width, label=col, \n                       color=CATEGORY_COLORS_A[col], \n                       edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH)\n\n# Add confidence intervals\nfor i, col in enumerate(LEGEND_ORDER_A):\n    for j, verdict in enumerate(pivot_flip.index):\n        csv_col = col_to_csv.get(col, col)\n        csv_verdict = label_to_csv.get(verdict, verdict)\n        mask = (flip_rates['base_verdict'] == csv_verdict) & \\\n               (flip_rates['perturbation_family'] == csv_col)\n        row = flip_rates[mask]\n        if not row.empty:\n            n = row['n_total'].values[0]\n            p = row['flip_rate'].values[0] / 100\n            se = np.sqrt(p * (1 - p) / n) * 100\n            ci = z * se\n            offset = (i - 1) * width\n            ax1_alt.errorbar(j + offset, pivot_flip.loc[verdict, col], yerr=ci, \n                             fmt='none', ecolor='black', capsize=ERROR_CAPSIZE, \n                             lw=ERROR_LINEWIDTH, capthick=ERROR_LINEWIDTH, zorder=10)\n\nax1_alt.set_xlabel('Base Verdict Category', fontweight='bold')\nax1_alt.set_ylabel('Flip Rate (%)', fontweight='bold')\nax1_alt.set_title('(A) Verdict Instability by Base Judgment', fontweight='bold', loc='left')\nax1_alt.set_xticks(x)\nax1_alt.set_xticklabels(pivot_flip.index, fontsize=9)\nax1_alt.legend(loc='upper right', framealpha=0.9, bbox_to_anchor=(1, 0.97))\nax1_alt.set_ylim(0, 65)\nax1_alt.grid(axis='y', alpha=0.3, linestyle='--')\nax1_alt.spines['top'].set_visible(False)\nax1_alt.spines['right'].set_visible(False)\n\n# =============================================================================\n# PANEL B: Blame Direction Ratio by Perturbation\n# =============================================================================\ny_positions = np.arange(len(blame_data))\ncolors_b2 = [CATEGORY_STYLES[cat]['color'] for cat in blame_data['category']]\nhatches_b2 = [CATEGORY_STYLES[cat]['hatch'] for cat in blame_data['category']]\n\n# Compute error bar asymmetric widths (for log-scale ratios)\nxerr_lower = blame_data['ratio'] - blame_data['ci_lower']\nxerr_upper = blame_data['ci_upper'] - blame_data['ratio']\nxerr = [xerr_lower.values, xerr_upper.values]\n\n# Horizontal bar chart\nbars = ax2_alt.barh(y_positions, blame_data['ratio'], \n                    color=colors_b2, edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH,\n                    height=0.7, xerr=xerr, capsize=ERROR_CAPSIZE,\n                    error_kw={'lw': ERROR_LINEWIDTH, 'capthick': ERROR_LINEWIDTH})\n\n# Apply hatching\nfor bar, hatch in zip(bars, hatches_b2):\n    bar.set_hatch(hatch)\n\n# Reference line at ratio = 1 (balanced)\nax2_alt.axvline(x=1.0, color='#222222', linewidth=1.2, linestyle='-', zorder=0)\n\n# Labels\nax2_alt.set_yticks(y_positions)\nax2_alt.set_yticklabels(blame_data['perturbation'])\nax2_alt.set_xlabel('Blame Direction Ratio (toward blame / toward exoneration)', fontweight='bold')\nax2_alt.set_title('(B) Asymmetric Perturbation Effectiveness', fontweight='bold', loc='left')\n\n# Category separators\nprev_cat = None\nfor i, (_, row) in enumerate(blame_data.iterrows()):\n    if row['category'] != prev_cat and prev_cat is not None:\n        ax2_alt.axhline(y=i-0.5, color='#cccccc', linewidth=0.5, linestyle='--')\n    prev_cat = row['category']\n\n# Annotate with ratio values only (no glyphs)\nfor i, (_, row) in enumerate(blame_data.iterrows()):\n    ratio = row['ratio']\n    # Position annotation after error bar\n    x_pos = row['ci_upper'] + 0.15\n    ax2_alt.annotate(f\"{ratio:.2f}x\",\n                     xy=(x_pos, i),\n                     fontsize=9, va='center', ha='left', color='#333333')\n\n# Legend - position at lower right to avoid overlap with top bars\nlegend_patches = [mpatches.Patch(facecolor=CATEGORY_STYLES[cat]['color'], \n                                  edgecolor=BAR_EDGECOLOR, linewidth=BAR_LINEWIDTH,\n                                  hatch=CATEGORY_STYLES[cat]['hatch'], label=cat)\n                  for cat in ['Persuasion: Push Self At Fault', 'Persuasion: Push Other At Fault', \n                              'Point-of-view', 'Surface']]\nax2_alt.legend(handles=legend_patches, loc='lower right', framealpha=0.9, fontsize=8)\n\n# Styling\nax2_alt.set_xlim(0, max(blame_data['ci_upper'].max() * 1.4, 7))\nax2_alt.spines['top'].set_visible(False)\nax2_alt.spines['right'].set_visible(False)\nax2_alt.invert_yaxis()\nax2_alt.grid(axis='x', alpha=0.3, linestyle='--')\n\n# Directional labels\nax2_alt.text(0.3, -0.7, '$\\\\leftarrow$ Exonerates narrator', fontsize=8, ha='left', style='italic', color='#666666')\nax2_alt.text(ax2_alt.get_xlim()[1] * 0.95, -0.7, 'Blames narrator $\\\\rightarrow$', fontsize=8, ha='right', style='italic', color='#666666')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jtzwdsw6nth",
   "source": "# Save alternative figure with blame direction panel\npdf_path_alt = output_dir / 'fig_instability_blame_direction_combined.pdf'\npng_path_alt = output_dir / 'fig_instability_blame_direction_combined.png'\n\nfig2.savefig(pdf_path_alt, bbox_inches='tight', dpi=300)\nfig2.savefig(png_path_alt, bbox_inches='tight', dpi=300)\n\nprint(f\"Saved: {pdf_path_alt}\")\nprint(f\"Saved: {png_path_alt}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tuxgjqk47g",
   "source": "print(\"KEY INSIGHTS - Alternative Panel B (Blame Direction)\")\nprint(\"=\" * 80)\n\nprint(\"\\nPanel B (Asymmetric Perturbation Effectiveness):\")\nprint(\"  - Persuasion: Push Self At Fault perturbations WORK as intended:\")\nfor _, row in blame_data[blame_data['category'] == 'Persuasion: Push Self At Fault'].iterrows():\n    print(f\"      {row['perturbation']}: {row['ratio']:.2f}x toward blame\")\n\nprint(\"\\n  - Persuasion: Push Other At Fault perturbations FAIL or BACKFIRE:\")\nfor _, row in blame_data[blame_data['category'] == 'Persuasion: Push Other At Fault'].iterrows():\n    status = \"BACKFIRE\" if row['ratio'] > 1.0 else \"weak\"\n    print(f\"      {row['perturbation']}: {row['ratio']:.2f}x ({status})\")\n\nprint(\"\\n  - Point-of-view perturbations show bias toward blame:\")\nfor _, row in blame_data[blame_data['category'] == 'Point-of-view'].iterrows():\n    print(f\"      {row['perturbation']}: {row['ratio']:.2f}x\")\n\nprint(\"\\n  - Surface perturbations are relatively balanced:\")\nfor _, row in blame_data[blame_data['category'] == 'Surface'].iterrows():\n    print(f\"      {row['perturbation']}: {row['ratio']:.2f}x\")\n\nprint(\"\\nKEY FINDING:\")\nprint(\"  Self-criticism is trusted (Push Self At Fault works: ~4-6x toward blame)\")\nprint(\"  Self-justification backfires (Push Other At Fault 'Self-justifying': >1x toward blame)\")\nprint(\"  This supports the 'credibility heuristic' interpretation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "j513spjzmhp",
   "source": "## Comparison: Epistemic Stance vs Blame Direction\n\n| Aspect | Epistemic Stance (Original) | Blame Direction (Alternative) |\n|--------|----------------------------|------------------------------|\n| **Shows** | How *language* changes | How *judgments* change |\n| **Unit** | Δ markers per 100 words | Ratio of transitions |\n| **Key finding** | Third-person → confident | Push Self At Fault works, Push Other At Fault backfires |\n| **Connects to paper narrative** | Secondary (linguistic) | Core (credibility heuristic) |\n| **Visual highlight** | Point-of-view = most confident | Self-justifying backfire |\n\n**Recommendation**: The blame direction panel directly visualizes the paper's core finding about asymmetric perturbation effectiveness and the credibility heuristic. The epistemic stance panel is interesting but tangential.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}